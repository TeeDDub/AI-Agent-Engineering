# fine_tune_function_calling.py
"""LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ LLM íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ëŠ” ê¹”ë”í•˜ê³  ëª¨ë“ˆí™”ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Bonus Unit ë…¸íŠ¸ë¶ì˜ ë‹¨ê³„ë“¤ì„ í•˜ë‚˜ì˜ Python ì§„ì…ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.
ê¸°ë³¸ ì‚¬ìš©ë²•:

    HF_TOKEN=<your_token> python fine_tune_function_calling.py \
        --model google/gemma-2-2b-it \
        --dataset Jofthomas/hermes-function-calling-thinking-V1 \
        --output_dir gemma-2-2B-function-call-ft

ëª¨ë“  ì˜µì…˜ì„ ë³´ë ¤ë©´ `--help`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
"""
from __future__ import annotations

import argparse
import os
import platform
from enum import Enum
from functools import partial
from pathlib import Path
from typing import Dict, List, Tuple

import torch
from datasets import DatasetDict, load_dataset
from peft import LoraConfig, PeftConfig, PeftModel, TaskType
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTConfig, SFTTrainer

###############################################################################
# íŠ¹ìˆ˜ í† í° ë° ì±„íŒ… í…œí”Œë¦¿ í—¬í¼
###############################################################################

class ChatmlSpecialTokens(str, Enum):
    tools = "<tools>"
    eotools = "</tools>"
    think = "<think>"
    eothink = "</think>"
    tool_call = "<tool_call>"
    eotool_call = "</tool_call>"
    tool_response = "<tool_response>"
    eotool_response = "</tool_response>"
    pad_token = "<pad>"
    eos_token = "<eos>"

    @classmethod
    def list(cls) -> List[str]:
        return [c.value for c in cls]

CHAT_TEMPLATE = (
    "{{ bos_token }}"
    "{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}"
    "{% for message in messages %}"
    "{{ '<start_of_turn>' + message['role'] + '\n' + message['content']|trim + '<end_of_turn><eos>\n' }}"
    "{% endfor %}"
    "{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
)

###############################################################################
# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
###############################################################################

def _merge_system_into_first_user(messages: List[Dict[str, str]]) -> None:
    """ì„ í–‰ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ í›„ì† ì‚¬ìš©ì ë©”ì‹œì§€ì— ë³‘í•©í•©ë‹ˆë‹¤."""
    if messages and messages[0]["role"] == "system":
        system_content = messages[0]["content"]
        messages.pop(0)
        if not messages or messages[0]["role"] != "human":
            raise ValueError("ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë‹¤ìŒì— ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        messages[0][
            "content"
        ] = (
            f"{system_content}ë˜í•œ, í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— ì‹œê°„ì„ ê°–ê³  "
            "í˜¸ì¶œí•  í•¨ìˆ˜ë¥¼ ê³„íší•˜ì„¸ìš”. ìƒê°í•˜ëŠ” ê³¼ì •ì„ "
            "<think>{your thoughts}</think> ì‚¬ì´ì— ì‘ì„±í•˜ì„¸ìš”.\n\n" + messages[0]["content"]
        )


def build_preprocess_fn(tokenizer):
    """ì›ì‹œ ìƒ˜í”Œì„ í† í¬ë‚˜ì´ì¦ˆëœ í”„ë¡¬í”„íŠ¸ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    def _preprocess(sample):
        messages = sample["messages"].copy()
        _merge_system_into_first_user(messages)
        prompt = tokenizer.apply_chat_template(messages, tokenize=False)
        return {"text": prompt}

    return _preprocess


def load_and_prepare_dataset(ds_name: str, tokenizer, max_train: int, max_eval: int) -> DatasetDict:
    """ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ì ìš©í•©ë‹ˆë‹¤."""
    raw = load_dataset(ds_name).rename_column("conversations", "messages")
    processed = raw.map(build_preprocess_fn(tokenizer), remove_columns="messages")
    split = processed["train"].train_test_split(test_size=0.1, seed=42)
    split["train"] = split["train"].select(range(max_train))
    split["test"] = split["test"].select(range(max_eval))
    return split

###############################################################################
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € í—¬í¼
###############################################################################

def build_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        pad_token=ChatmlSpecialTokens.pad_token.value,
        additional_special_tokens=ChatmlSpecialTokens.list(),
    )
    tokenizer.chat_template = CHAT_TEMPLATE
    return tokenizer


def build_model(model_name: str, tokenizer, load_4bit: bool = False):
    """ëª¨ë¸ ë¡œë“œ. Macì—ì„œëŠ” bitsandbytesê°€ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ 4bit ì–‘ìí™” ë¹„í™œì„±í™”."""
    kwargs = {
        "attn_implementation": "eager",  # flash_attn í˜¸í™˜ì„± (Phi-3 window_size ë“±)
        "device_map": "auto",
        "torch_dtype": torch.bfloat16,
    }
    # load_4bit=Trueì´ê³  Macì´ ì•„ë‹ ë•Œë§Œ 4bit ì–‘ìí™” ì‚¬ìš© (bitsandbytesëŠ” Macì—ì„œ ë¶ˆì•ˆì •)
    use_4bit = load_4bit and platform.system() != "Darwin"
    if use_4bit:
        kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.resize_token_embeddings(len(tokenizer))
    return model

###############################################################################
# PEFT / LoRA í—¬í¼
###############################################################################

def build_lora_config(r: int = 16, alpha: int = 64, dropout: float = 0.05) -> LoraConfig:
    return LoraConfig(
        r=r,
        lora_alpha=alpha,
        lora_dropout=dropout,
        target_modules=[
            "gate_proj",
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "up_proj",
            "down_proj",
            "lm_head",
            "embed_tokens",
        ],
        task_type=TaskType.CAUSAL_LM,
    )

###############################################################################
# í•™ìŠµ
###############################################################################

def train(
    model,
    tokenizer,
    dataset: DatasetDict,
    peft_cfg: LoraConfig,
    output_dir: str,
    epochs: int = 1,
    lr: float = 1e-4,
    batch_size: int = 1,
    grad_accum: int = 4,
    max_seq_len: int = 1500,
):
    train_args = SFTConfig(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=grad_accum,
        save_strategy="no",
        eval_strategy="epoch",
        logging_steps=5,
        learning_rate=lr,
        num_train_epochs=epochs,
        max_grad_norm=1.0,
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",
        report_to=None,
        bf16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        packing=True,
    )

    trainer = SFTTrainer(
        model=model,
        args=train_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        processing_class=tokenizer,
        peft_config=peft_cfg,
    )

    trainer.train()
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    return trainer

###############################################################################
# ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤(CLI)
###############################################################################

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ LLM íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    parser.add_argument("--model", default="microsoft/Phi-3-mini-4k-instruct", help="ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ")
    parser.add_argument("--dataset", default="Jofthomas/hermes-function-calling-thinking-V1", help="HuggingFace ë°ì´í„°ì…‹")
    parser.add_argument("--output_dir", default="ch07/fine_tuned_model/gemma-2-2B-function-call-ft", help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--grad_accum", type=int, default=4)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--max_train", type=int, default=100, help="ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--max_eval", type=int, default=10, help="ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ í‰ê°€ ë°ì´í„° í–‰ ìˆ˜")
    parser.add_argument("--push_to_hub", action="store_true")
    parser.add_argument("--hf_username", default=None, help="ëª¨ë¸ í‘¸ì‹œë¥¼ ìœ„í•œ HuggingFace ì‚¬ìš©ìëª…")
    parser.add_argument("--load_4bit", action="store_true", help="4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë“œë¡œ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ")
    return parser.parse_args()


def maybe_push_to_hub(trainer: SFTTrainer, tokenizer, username: str, output_dir: str):
    if not username:
        print("HuggingFace ì‚¬ìš©ìëª…ì´ ì œê³µë˜ì§€ ì•Šì•„ push_to_hubì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    repo = f"{username}/{Path(output_dir).name}"
    print(f"\nì–´ëŒ‘í„° ë° í† í¬ë‚˜ì´ì €ë¥¼ https://huggingface.co/{repo} ì— í‘¸ì‹œí•˜ëŠ” ì¤‘ â€¦")
    trainer.push_to_hub(repo)
    tokenizer.push_to_hub(repo, token=os.environ.get("HF_TOKEN"))

###############################################################################
# ì§„ì…ì 
###############################################################################

def main():
    args = parse_args()

    tokenizer = build_tokenizer(args.model)
    model = build_model(args.model, tokenizer, load_4bit=args.load_4bit)

    dataset = load_and_prepare_dataset(
        args.dataset, tokenizer, max_train=args.max_train, max_eval=args.max_eval
    )

    lora_cfg = build_lora_config()
    results = train(
        model,
        tokenizer,
        dataset,
        lora_cfg,
        output_dir=args.output_dir,
        epochs=args.epochs,
        lr=args.lr,
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
    )

    print("\ní•™ìŠµ ì™„ë£Œ! ğŸ‰")
    print(results)


if __name__ == "__main__":
    main()
