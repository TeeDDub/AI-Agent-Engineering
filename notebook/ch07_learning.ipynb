{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 07: í•™ìŠµ (Learning)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” Reflexion íŒ¨í„´ì„ í†µí•œ ì—ì´ì „íŠ¸ ìê¸° ê°œì„ ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” ë‚´ìš©\n",
        "- Reflexion íŒ¨í„´\n",
        "- ì‹¤íŒ¨ ê¸°ë°˜ ê°œì„ \n",
        "- ê²½í—˜ì  í•™ìŠµ\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/Building-Applications-with-AI-Agents/blob/main/notebook/ch07_learning.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langgraph python-dotenv datasets torch transformers peft trl bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API í‚¤ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"âœ… Colab Secretsì—ì„œ API í‚¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
        "    print(\"âš ï¸ API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. reflexion.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reflexion íŒ¨í„´ìœ¼ë¡œ ì‹¤íŒ¨ ë¶„ì„ ë° ê°œì„  ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ í™•ì¸\n",
        "import os\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  \n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        \"í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .env íŒŒì¼ì—ì„œ ì„¤ì •í•´ì£¼ì„¸ìš”.\"\n",
        "    )\n",
        "\n",
        "# LLM ì´ˆê¸°í™”\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", temperature=0)\n",
        "\n",
        "reflections = []\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "reflexion_prompt = f\"\"\"ì–´ë–¤ í™˜ê²½ì— ë†“ì¸ ë’¤ íŠ¹ì • ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ë¼ëŠ” ì§€ì‹œë¥¼ ë°›ì•˜ë˜ ê³¼ê±° ê²½í—˜ì˜ ê¸°ë¡ì´ ì£¼ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤.\n",
        "ë‹¹ì‹ ì€ ê·¸ ê³¼ì œë¥¼ ì™„ìˆ˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ì„ ìš”ì•½í•˜ë ¤ê³  í•˜ì§€ ë§ê³ , ê³¼ì œë¥¼ ì™„ìˆ˜í•˜ê¸° ìœ„í•´ ì‹œë„í–ˆë˜ ì „ëµê³¼ ì§„í–‰ ê²½ë¡œì— ëŒ€í•´ ìƒê°í•´ ë³´ì‹­ì‹œì˜¤.\n",
        "ë‹¹ì‹ ì´ í–ˆì–´ì•¼ í•˜ì§€ë§Œ í•˜ì§€ ëª»í–ˆë˜ êµ¬ì²´ì ì¸ í–‰ë™ë“¤ì„ ì°¸ê³ í•˜ì—¬, ê·¸ ì‹¤ìˆ˜ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ê°„ê²°í•˜ê³  ìƒˆë¡œìš´ ì‹¤í–‰ ê³„íšì„ ì„¸ìš°ì‹­ì‹œì˜¤.\n",
        "ì˜ˆë¥¼ ë“¤ì–´, Aì™€ BëŠ” ì‹œë„í–ˆì§€ë§Œ Cë¥¼ ìŠì–´ë²„ë ¸ë‹¤ë©´, í•´ë‹¹ í™˜ê²½ì—ì„œ Cë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì–´ë–¤ í–‰ë™ì„ í–ˆì–´ì•¼ í•˜ëŠ”ì§€ í™˜ê²½ì— íŠ¹í™”ëœ í–‰ë™ë“¤ë¡œ ê³„íšì„ ì„¸ìš°ë©´ ë©ë‹ˆë‹¤.\n",
        "ì´ ê³„íšì€ ë‚˜ì¤‘ì— ê°™ì€ ê³¼ì œë¥¼ ë‹¤ì‹œ í’€ ë•Œ í•„ìš”í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. \"Plan\"ì´ë¼ëŠ” ë‹¨ì–´ ë’¤ì— ìì‹ ì˜ ê³„íšì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
        "\n",
        "Instruction:\n",
        "ì €ëŠ” ìœ ì œí’ˆì´ ë“¤ì–´ ìˆì§€ ì•Šê³  ì‚¬ê³¼ ë§›ì´ í¬í•¨ëœ ì¹©ìŠ¤ ë²„ë¼ì´ì–´í‹° íŒ©ì„ ì°¾ê³  ìˆìœ¼ë©°, ê°€ê²©ì€ 30ë‹¬ëŸ¬ ë¯¸ë§Œì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "[Search]\n",
        "\n",
        "Action: search[dairy free and apple variety pack of chips]\n",
        "Observation:\n",
        "[ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸°]\n",
        "í˜ì´ì§€ 1 (ì´ ê²°ê³¼: 50)\n",
        "[ë‹¤ìŒ >]\n",
        "[B07HRFJWP8]\n",
        "Enjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free, Non GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes (20 Bars Total)\n",
        "$100.0\n",
        "[B01KMHY5PG]\n",
        "Enjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total Bars)\n",
        "$21.49\n",
        "[B008D2X8C4]\n",
        "Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags (0.8 oz)\n",
        "$100.0\n",
        "\n",
        "Action: think[ì•ì˜ ë‘ ìƒí’ˆì€ ì œê°€ ì›í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ ìƒí’ˆì´ ì œê°€ ì°¾ëŠ”, ìœ ì œí’ˆì´ ë“¤ì–´ ìˆì§€ ì•Šê³  ì‚¬ê³¼ ë§›ì´ í¬í•¨ëœ ì¹©ìŠ¤ ë²„ë¼ì´ì–´í‹° íŒ©ì…ë‹ˆë‹¤.]\n",
        "Observation: ì•Œê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "Action: click[B008D2X8C4]\n",
        "Observation:\n",
        "[ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸°]\n",
        "[< ì´ì „]\n",
        "flavor name [dill and sour cream][garlic & parmesan][light sea salt][margherita pizza][thai chili lime][variety pack]\n",
        "size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]\n",
        "ê°€ê²©(Price): $100.0\n",
        "í‰ì (Rating): N.A.\n",
        "[Description]\n",
        "[Features]\n",
        "[Reviews]\n",
        "[Buy Now]\n",
        "\n",
        "Action: think[ì´ ìƒí’ˆì—ëŠ” 'variety pack'ê³¼ '0.8 ounce (pack of 24)' ì˜µì…˜ì´ ìˆê³ , êµ¬ë§¤í•˜ê¸°ì— ì¢‹ì•„ ë³´ì…ë‹ˆë‹¤.]\n",
        "Observation: ì•Œê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "Action: click[variety pack]\n",
        "Observation: variety packì„ í´ë¦­í•˜ì…¨ìŠµë‹ˆë‹¤.\n",
        "\n",
        "Action: click[0.8 ounce (pack of 24)]\n",
        "Observation: 0.8 ounce (pack of 24)ë¥¼ í´ë¦­í•˜ì…¨ìŠµë‹ˆë‹¤.\n",
        "\n",
        "Action: click[Buy Now]\n",
        "\n",
        "STATUS: FAIL\n",
        "\n",
        "Plan:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"reflexion\", call_model)\n",
        "builder.add_edge(START, \"reflexion\")\n",
        "graph = builder.compile()\n",
        "\n",
        "result = graph.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                reflexion_prompt\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "reflections.append(result)\n",
        "\n",
        "# ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”„ Reflexion ê²°ê³¼\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for i, msg in enumerate(result[\"messages\"]):\n",
        "    msg_type = msg.__class__.__name__\n",
        "    \n",
        "    if msg_type == \"HumanMessage\":\n",
        "        print(f\"ì…ë ¥ ë©”ì‹œì§€:\")\n",
        "        print(\"-\" * 80)\n",
        "        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ\n",
        "        content = msg.content\n",
        "        if len(content) > 500:\n",
        "            print(content[:250] + \"\\n\\n... (ì¤‘ëµ) ...\\n\\n\" + content[-250:])\n",
        "        else:\n",
        "            print(content)\n",
        "        print()\n",
        "        \n",
        "    elif msg_type == \"AIMessage\":\n",
        "        print(f\"AI ì‘ë‹µ:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(msg.content)\n",
        "        print()\n",
        "\n",
        "print(\"=\"*80)",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. experiential_learning.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ê²½í—˜ ê¸°ë°˜ í•™ìŠµ íë¦„ì„ LangGraphë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ í™•ì¸\n",
        "import os\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  \n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        \"í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .env íŒŒì¼ì—ì„œ ì„¤ì •í•´ì£¼ì„¸ìš”.\"\n",
        "    )\n",
        "\n",
        "# LLM ì´ˆê¸°í™”\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", temperature=0)\n",
        "\n",
        "# LLM í˜¸ì¶œ í•¨ìˆ˜\n",
        "def call_model(state: MessagesState):\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "class InsightAgent:\n",
        "    def __init__(self):\n",
        "        self.insights = []\n",
        "        self.promoted_insights = []\n",
        "        self.demoted_insights = []\n",
        "        self.reflections = []\n",
        "\n",
        "    def generate_insight(self, observation):\n",
        "        # LLMì„ ì‚¬ìš©í•˜ì—¬ ê´€ì°°ì— ê¸°ë°˜í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "        messages = [HumanMessage(content=f\"ë‹¤ìŒ ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”: '{observation}'\")]\n",
        "\n",
        "        # ìƒíƒœ ê·¸ë˜í”„ ìƒì„±\n",
        "        builder = StateGraph(MessagesState)\n",
        "        builder.add_node(\"generate_insight\", call_model)\n",
        "        builder.add_edge(START, \"generate_insight\")\n",
        "        graph = builder.compile()\n",
        "\n",
        "        # ë©”ì‹œì§€ì™€ í•¨ê»˜ ê·¸ë˜í”„ í˜¸ì¶œ\n",
        "        result = graph.invoke({\"messages\": messages})\n",
        "\n",
        "        # ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ\n",
        "        generated_insight = result[\"messages\"][-1].content\n",
        "        self.insights.append(generated_insight)\n",
        "        print(f\"ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {generated_insight}\")\n",
        "        return generated_insight\n",
        "\n",
        "    def promote_insight(self, insight):\n",
        "        if insight in self.insights:\n",
        "            self.insights.remove(insight)\n",
        "            self.promoted_insights.append(insight)\n",
        "            print(f\"ìŠ¹ê²©ëœ ì¸ì‚¬ì´íŠ¸: {insight}\")\n",
        "        else:\n",
        "            print(f\"'{insight}'ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    def demote_insight(self, insight):\n",
        "        if insight in self.promoted_insights:\n",
        "            self.promoted_insights.remove(insight)\n",
        "            self.demoted_insights.append(insight)\n",
        "            print(f\"ê°•ë“±ëœ ì¸ì‚¬ì´íŠ¸: {insight}\")\n",
        "        else:\n",
        "            print(f\"'{insight}'ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    def edit_insight(self, old_insight, new_insight):\n",
        "        # ëª¨ë“  ë¦¬ìŠ¤íŠ¸ì—ì„œ í™•ì¸\n",
        "        if old_insight in self.insights:\n",
        "            index = self.insights.index(old_insight)\n",
        "            self.insights[index] = new_insight\n",
        "        elif old_insight in self.promoted_insights:\n",
        "            index = self.promoted_insights.index(old_insight)\n",
        "            self.promoted_insights[index] = new_insight\n",
        "        elif old_insight in self.demoted_insights:\n",
        "            index = self.demoted_insights.index(old_insight)\n",
        "            self.demoted_insights[index] = new_insight\n",
        "        else:\n",
        "            print(f\"'{old_insight}'ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return\n",
        "        print(f\"ìˆ˜ì •ëœ ì¸ì‚¬ì´íŠ¸: '{old_insight}' -> '{new_insight}'\")\n",
        "\n",
        "    def show_insights(self):\n",
        "        print(\"\\ní˜„ì¬ ì¸ì‚¬ì´íŠ¸:\")\n",
        "        print(f\"ì¸ì‚¬ì´íŠ¸: {self.insights}\")\n",
        "        print(f\"ìŠ¹ê²©ëœ ì¸ì‚¬ì´íŠ¸: {self.promoted_insights}\")\n",
        "        print(f\"ê°•ë“±ëœ ì¸ì‚¬ì´íŠ¸: {self.demoted_insights}\")\n",
        "\n",
        "    def reflect(self, reflexion_prompt):\n",
        "        # ì„±ì°°ì„ ìœ„í•œ ìƒíƒœ ê·¸ë˜í”„ ìƒì„±\n",
        "        builder = StateGraph(MessagesState)\n",
        "        builder.add_node(\"reflection\", call_model)\n",
        "        builder.add_edge(START, \"reflection\")\n",
        "        graph = builder.compile()\n",
        "\n",
        "        # ì„±ì°° í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ê·¸ë˜í”„ í˜¸ì¶œ\n",
        "        result = graph.invoke(\n",
        "            {\n",
        "                \"messages\": [\n",
        "                    HumanMessage(\n",
        "                        content=reflexion_prompt\n",
        "                    )\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "        reflection = result[\"messages\"][-1].content\n",
        "        self.reflections.append(reflection)\n",
        "        print(f\"ì„±ì°°: {reflection}\")\n",
        "\n",
        "agent = InsightAgent()\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ëœ ê´€ì°° ì‹œí€€ìŠ¤ì™€ KPI íƒ€ê²Ÿ ë‹¬ì„± ì—¬ë¶€\n",
        "reports = [\n",
        "    (\"ì›¹ì‚¬ì´íŠ¸ íŠ¸ë˜í”½ì´ 15% ì¦ê°€í–ˆì§€ë§Œ, ë°”ìš´ìŠ¤ìœ¨ì´ 40%ì—ì„œ 55%ë¡œ ê¸‰ê²©íˆ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.\", \n",
        "        False),\n",
        "    (\"ì´ë©”ì¼ ì—´ëŒë¥ ì´ 25%ë¡œ í–¥ìƒë˜ì—ˆì§€ë§Œ, 20% ëª©í‘œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\", True),\n",
        "    (\"ì¥ë°”êµ¬ë‹ˆ í¬ê¸°ìœ¨ì´ 60%ì—ì„œ 68%ë¡œ ì¦ê°€í–ˆì§€ë§Œ, 50% ëª©í‘œë¥¼ ë†“ì³¤ìŠµë‹ˆë‹¤.\", \n",
        "        False),\n",
        "    (\"í‰ê·  ì£¼ë¬¸ ê°€ì¹˜ê°€ 8% ì¦ê°€í–ˆì§€ë§Œ, 5% ì¦ê°€ ëª©í‘œë¥¼ ë†“ì³¤ìŠµë‹ˆë‹¤.\", True),\n",
        "    (\"ì‹ ê·œ êµ¬ë…ì ìˆ˜ê°€ 5% ê°ì†Œí–ˆì§€ë§Œ, 10% ì„±ì¥ ëª©í‘œë¥¼ ë†“ì³¤ìŠµë‹ˆë‹¤.\", \n",
        "        False),\n",
        "]\n",
        "# 1) ë³´ê³ ì„œ ê¸°ê°„ ë™ì•ˆ ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° ìš°ì„ ìˆœìœ„ ì§€ì •\n",
        "for text, hit_target in reports:\n",
        "    insight = agent.generate_insight(text)\n",
        "    if hit_target:\n",
        "        agent.promote_insight(insight)\n",
        "    else:\n",
        "        agent.demote_insight(insight)\n",
        "# 2) ìŠ¹ê²©ëœ ì¸ì‚¬ì´íŠ¸ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ëŒì´ ì°¸ì—¬í•˜ëŠ” í¸ì§‘ìœ¼ë¡œ ê°œì„ \n",
        "if agent.promoted_insights:\n",
        "    original = agent.promoted_insights[0]\n",
        "    agent.edit_insight(original, f'ê°œì„ ëœ ì¸ì‚¬ì´íŠ¸: {original} ë°©ë¬¸ì ê²½í—˜ ê°œì„ ì„ ìœ„í•œ ëœë”© í˜ì´ì§€ UX ë³€ê²½ ì¡°ì‚¬')\n",
        "# 3) ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒíƒœ í‘œì‹œ\n",
        "agent.show_insights()\n",
        "# 4) ìµœìƒìœ„ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê³„íš ì„±ì°°\n",
        "reflection_prompt = (\n",
        "    \"ìŠ¹ê²©ëœ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ë¶„ê¸°ì— ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ì˜ ê³ ì˜í–¥ ì‹¤í—˜ì„ ì œì•ˆí•˜ì„¸ìš”:\" + f\"\\n{agent.promoted_insights}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. supervised_fine_tuning.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SFT íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fine_tune_function_calling.py\n",
        "\"\"\"LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ LLM íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ëŠ” ê¹”ë”í•˜ê³  ëª¨ë“ˆí™”ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Bonus Unit ë…¸íŠ¸ë¶ì˜ ë‹¨ê³„ë“¤ì„ í•˜ë‚˜ì˜ Python ì§„ì…ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
        "ê¸°ë³¸ ì‚¬ìš©ë²•:\n",
        "\n",
        "    HF_TOKEN=<your_token> python fine_tune_function_calling.py \\\n",
        "        --model google/gemma-2-2b-it \\\n",
        "        --dataset Jofthomas/hermes-function-calling-thinking-V1 \\\n",
        "        --output_dir gemma-2-2B-function-call-ft\n",
        "\n",
        "ëª¨ë“  ì˜µì…˜ì„ ë³´ë ¤ë©´ `--help`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "from datasets import DatasetDict, load_dataset\n",
        "from peft import LoraConfig, PeftConfig, PeftModel, TaskType\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "###############################################################################\n",
        "# íŠ¹ìˆ˜ í† í° ë° ì±„íŒ… í…œí”Œë¦¿ í—¬í¼\n",
        "###############################################################################\n",
        "\n",
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    tools = \"<tools>\"\n",
        "    eotools = \"</tools>\"\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    tool_call = \"<tool_call>\"\n",
        "    eotool_call = \"</tool_call>\"\n",
        "    tool_response = \"<tool_response>\"\n",
        "    eotool_response = \"</tool_response>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls) -> List[str]:\n",
        "        return [c.value for c in cls]\n",
        "\n",
        "CHAT_TEMPLATE = (\n",
        "    \"{{ bos_token }}\"\n",
        "    \"{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}\"\n",
        "    \"{% for message in messages %}\"\n",
        "    \"{{ '<start_of_turn>' + message['role'] + '\\n' + message['content']|trim + '<end_of_turn><eos>\\n' }}\"\n",
        "    \"{% endfor %}\"\n",
        "    \"{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        ")\n",
        "\n",
        "###############################################################################\n",
        "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
        "###############################################################################\n",
        "\n",
        "def _merge_system_into_first_user(messages: List[Dict[str, str]]) -> None:\n",
        "    \"\"\"ì„ í–‰ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ í›„ì† ì‚¬ìš©ì ë©”ì‹œì§€ì— ë³‘í•©í•©ë‹ˆë‹¤.\"\"\"\n",
        "    if messages and messages[0][\"role\"] == \"system\":\n",
        "        system_content = messages[0][\"content\"]\n",
        "        messages.pop(0)\n",
        "        if not messages or messages[0][\"role\"] != \"human\":\n",
        "            raise ValueError(\"ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë‹¤ìŒì— ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "        messages[0][\n",
        "            \"content\"\n",
        "        ] = (\n",
        "            f\"{system_content}ë˜í•œ, í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— ì‹œê°„ì„ ê°–ê³  \"\n",
        "            \"í˜¸ì¶œí•  í•¨ìˆ˜ë¥¼ ê³„íší•˜ì„¸ìš”. ìƒê°í•˜ëŠ” ê³¼ì •ì„ \"\n",
        "            \"<think>{your thoughts}</think> ì‚¬ì´ì— ì‘ì„±í•˜ì„¸ìš”.\\n\\n\" + messages[0][\"content\"]\n",
        "        )\n",
        "\n",
        "\n",
        "def build_preprocess_fn(tokenizer):\n",
        "    \"\"\"ì›ì‹œ ìƒ˜í”Œì„ í† í¬ë‚˜ì´ì¦ˆëœ í”„ë¡¬í”„íŠ¸ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    def _preprocess(sample):\n",
        "        messages = sample[\"messages\"].copy()\n",
        "        _merge_system_into_first_user(messages)\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        return {\"text\": prompt}\n",
        "\n",
        "    return _preprocess\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(ds_name: str, tokenizer, max_train: int, max_eval: int) -> DatasetDict:\n",
        "    \"\"\"ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ì ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
        "    raw = load_dataset(ds_name).rename_column(\"conversations\", \"messages\")\n",
        "    processed = raw.map(build_preprocess_fn(tokenizer), remove_columns=\"messages\")\n",
        "    split = processed[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "    split[\"train\"] = split[\"train\"].select(range(max_train))\n",
        "    split[\"test\"] = split[\"test\"].select(range(max_eval))\n",
        "    return split\n",
        "\n",
        "###############################################################################\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € í—¬í¼\n",
        "###############################################################################\n",
        "\n",
        "def build_tokenizer(model_name: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "    )\n",
        "    tokenizer.chat_template = CHAT_TEMPLATE\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def build_model(model_name: str, tokenizer, load_4bit: bool = False):\n",
        "    kwargs = {\n",
        "        \"attn_implementation\": \"eager\",\n",
        "        \"device_map\": \"auto\",\n",
        "    }\n",
        "    kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    return model\n",
        "\n",
        "###############################################################################\n",
        "# PEFT / LoRA í—¬í¼\n",
        "###############################################################################\n",
        "\n",
        "def build_lora_config(r: int = 16, alpha: int = 64, dropout: float = 0.05) -> LoraConfig:\n",
        "    return LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        target_modules=[\n",
        "            \"gate_proj\",\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"lm_head\",\n",
        "            \"embed_tokens\",\n",
        "        ],\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "\n",
        "###############################################################################\n",
        "# í•™ìŠµ\n",
        "###############################################################################\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset: DatasetDict,\n",
        "    peft_cfg: LoraConfig,\n",
        "    output_dir: str,\n",
        "    epochs: int = 1,\n",
        "    lr: float = 1e-4,\n",
        "    batch_size: int = 1,\n",
        "    grad_accum: int = 4,\n",
        "    max_seq_len: int = 1500,\n",
        "):\n",
        "    train_args = SFTConfig(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=grad_accum,\n",
        "        save_strategy=\"no\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        logging_steps=5,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=epochs,\n",
        "        max_grad_norm=1.0,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        report_to=None,\n",
        "        bf16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        processing_class=tokenizer,\n",
        "        peft_config=peft_cfg,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    return trainer\n",
        "\n",
        "###############################################################################\n",
        "# ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤(CLI)\n",
        "###############################################################################\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ LLM íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
        "    parser.add_argument(\"--model\", default=\"microsoft/Phi-3-mini-4k-instruct\", help=\"ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ\")\n",
        "    parser.add_argument(\"--dataset\", default=\"Jofthomas/hermes-function-calling-thinking-V1\", help=\"HuggingFace ë°ì´í„°ì…‹\")\n",
        "    parser.add_argument(\"--output_dir\", default=\"ch07/fine_tuned_model/gemma-2-2B-function-call-ft\", help=\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--grad_accum\", type=int, default=4)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "    parser.add_argument(\"--max_train\", type=int, default=100, help=\"ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„° í–‰ ìˆ˜\")\n",
        "    parser.add_argument(\"--max_eval\", type=int, default=10, help=\"ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ í‰ê°€ ë°ì´í„° í–‰ ìˆ˜\")\n",
        "    parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n",
        "    parser.add_argument(\"--hf_username\", default=None, help=\"ëª¨ë¸ í‘¸ì‹œë¥¼ ìœ„í•œ HuggingFace ì‚¬ìš©ìëª…\")\n",
        "    parser.add_argument(\"--load_4bit\", action=\"store_true\", help=\"4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë“œë¡œ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def maybe_push_to_hub(trainer: SFTTrainer, tokenizer, username: str, output_dir: str):\n",
        "    if not username:\n",
        "        print(\"HuggingFace ì‚¬ìš©ìëª…ì´ ì œê³µë˜ì§€ ì•Šì•„ push_to_hubì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "        return\n",
        "    repo = f\"{username}/{Path(output_dir).name}\"\n",
        "    print(f\"\\nì–´ëŒ‘í„° ë° í† í¬ë‚˜ì´ì €ë¥¼ https://huggingface.co/{repo} ì— í‘¸ì‹œí•˜ëŠ” ì¤‘ â€¦\")\n",
        "    trainer.push_to_hub(repo)\n",
        "    tokenizer.push_to_hub(repo, token=os.environ.get(\"HF_TOKEN\"))\n",
        "\n",
        "###############################################################################\n",
        "# ì§„ì…ì \n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    tokenizer = build_tokenizer(args.model)\n",
        "    model = build_model(args.model, tokenizer, load_4bit=args.load_4bit)\n",
        "\n",
        "    dataset = load_and_prepare_dataset(\n",
        "        args.dataset, tokenizer, max_train=args.max_train, max_eval=args.max_eval\n",
        "    )\n",
        "\n",
        "    lora_cfg = build_lora_config()\n",
        "    results = train(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        dataset,\n",
        "        lora_cfg,\n",
        "        output_dir=args.output_dir,\n",
        "        epochs=args.epochs,\n",
        "        lr=args.lr,\n",
        "        batch_size=args.batch_size,\n",
        "        grad_accum=args.grad_accum,\n",
        "    )\n",
        "\n",
        "    print(\"\\ní•™ìŠµ ì™„ë£Œ! ğŸ‰\")\n",
        "    print(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. direct_preference_optimization.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DPOë¡œ ì„ í˜¸ë„ ê¸°ë°˜ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fine_tune_helpdesk_dpo.py\n",
        "# DPO(Direct Preference Optimization)ë¥¼ ì‚¬ìš©í•œ í—¬í”„ë°ìŠ¤í¬ ëª¨ë¸ íŒŒì¸íŠœë‹\n",
        "import torch, os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "import logging\n",
        "\n",
        "BASE_SFT_CKPT = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "DPO_DATA      = \"ch07/training_data/dpo_it_help_desk_training_data.jsonl\"                   # -> ê²½ë¡œ ë˜ëŠ” HF ë°ì´í„°ì…‹\n",
        "OUTPUT_DIR    = \"ch07/fine_tuned_model/phi3-mini-helpdesk-dpo\"\n",
        "\n",
        "# 1ï¸âƒ£ ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tok = AutoTokenizer.from_pretrained(BASE_SFT_CKPT, padding_side=\"right\",\n",
        "                                    trust_remote_code=True)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "if not os.path.exists(BASE_SFT_CKPT):\n",
        "    logger.warning(\"ë¡œì»¬ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Hubì—ì„œ '%s'ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\", BASE_SFT_CKPT)\n",
        "\n",
        "# 2ï¸âƒ£ ì–‘ìí™” ì„¤ì • (4ë¹„íŠ¸)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,               # ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”\n",
        "    bnb_4bit_use_double_quant=True,  # ì„ íƒì‚¬í•­: ì¤‘ì²© ì–‘ìí™”\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 3ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì ìš©)\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_SFT_CKPT,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# 4ï¸âƒ£ LoRA ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "   target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",\n",
        "                    \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "print(\"âœ…  Phi-3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:\", model.config.hidden_size, \"hidden dim\")\n",
        "\n",
        "# 5ï¸âƒ£ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "ds = load_dataset(\"json\", data_files=DPO_DATA, split=\"train\")\n",
        "\n",
        "# 6ï¸âƒ£ í•™ìŠµ ì¸ì ì„¤ì • (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, DPOConfigë¡œ ëŒ€ì²´)\n",
        "train_args = TrainingArguments(\n",
        "    output_dir      = OUTPUT_DIR,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    learning_rate   = 5e-6,\n",
        "    num_train_epochs= 3,\n",
        "    logging_steps   = 10,\n",
        "    save_strategy   = \"epoch\",\n",
        "    bf16            = True,\n",
        "    report_to       = None,\n",
        ")\n",
        "\n",
        "# 7ï¸âƒ£ DPO í•™ìŠµ ì„¤ì •\n",
        "dpo_args = DPOConfig(\n",
        "    output_dir              = \"phi3-mini-helpdesk-dpo\",\n",
        "    per_device_train_batch_size  = 4,\n",
        "    gradient_accumulation_steps  = 4,\n",
        "    learning_rate           = 5e-6,\n",
        "    num_train_epochs        = 3.0,\n",
        "    bf16                    = True,\n",
        "    logging_steps           = 10,\n",
        "    save_strategy           = \"epoch\",\n",
        "    report_to               = None,\n",
        "    beta                    = 0.1,\n",
        "    loss_type               = \"sigmoid\",\n",
        "    label_smoothing         = 0.0,\n",
        "    max_prompt_length       = 4096,\n",
        "    max_completion_length   = 4096,\n",
        "    max_length              = 8192,\n",
        "    padding_value           = tok.pad_token_id,\n",
        "    label_pad_token_id      = tok.pad_token_id,\n",
        "    truncation_mode         = \"keep_end\",\n",
        "    generate_during_eval    = False,\n",
        "    disable_dropout         = False,\n",
        "    reference_free          = True,\n",
        "    model_init_kwargs       = None,\n",
        "    ref_model_init_kwargs   = None,\n",
        ")\n",
        "\n",
        "# 8ï¸âƒ£ DPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,           # reference_free=Trueì´ë¯€ë¡œ ì°¸ì¡° ëª¨ë¸ ë¶ˆí•„ìš”\n",
        "    args=dpo_args,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "\n",
        "# 9ï¸âƒ£ í•™ìŠµ ì‹¤í–‰ ë° ëª¨ë¸ ì €ì¥\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "tok.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"âœ…  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. reinforcement_learning_with_verifiable_rewards.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ê¸°ë°˜ RL ì‹¤í—˜ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# RLVR(Reinforcement Learning with Verifiable Rewards) í›ˆë ¨ ë°ì´í„° ê²½ë¡œ\n",
        "DPO_DATA = \"ch07/training_data/rlvr_it_help_desk_training_data.jsonl\"\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset = load_dataset(\"json\", data_files=DPO_DATA, split=\"train\")\n",
        "\n",
        "def reward_tool_call_quality(completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    í•¨ìˆ˜ í˜¸ì¶œ í’ˆì§ˆì— ëŒ€í•œ ì„¸ë°€í•œ ë³´ìƒ í•¨ìˆ˜.\n",
        "\n",
        "    ë³´ìƒ ì²´ê³„:\n",
        "    - ì˜¬ë°”ë¥¸ ë„êµ¬ëª… + ìœ íš¨í•œ JSON + í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜: +1.0\n",
        "    - ì˜¬ë°”ë¥¸ ë„êµ¬ëª… + ìœ íš¨í•œ JSON + ëˆ„ë½ëœ ë§¤ê°œë³€ìˆ˜: +0.5\n",
        "    - ì˜¬ë°”ë¥¸ ë„êµ¬ëª… + ì˜ëª»ëœ JSON: +0.2\n",
        "    - ì˜ëª»ëœ ë„êµ¬ëª… + ìœ íš¨í•œ JSON: -0.3\n",
        "    - ë„êµ¬ í˜¸ì¶œ ì—†ìŒ ë˜ëŠ” ì™„ì „íˆ ì˜ëª»ë¨: -1.0\n",
        "    \"\"\"\n",
        "    labels = kwargs.get('label', [])\n",
        "    expected_params = kwargs.get('required_params', [])  # ì„ íƒì‚¬í•­: í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ì´ë¦„ ëª©ë¡\n",
        "\n",
        "    rewards = []\n",
        "    num_generations = kwargs.get('num_generations', getattr(trainer.args, 'num_generations', 1))\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        # ì´ ì™„ì„± ê²°ê³¼ì— ëŒ€í•œ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
        "        label_idx = i // num_generations\n",
        "        if label_idx >= len(labels):\n",
        "            rewards.append(-1.0)\n",
        "            continue\n",
        "\n",
        "        label = labels[label_idx]\n",
        "        expected_tool = label.lower().strip()\n",
        "\n",
        "        # ì™„ì„± ê²°ê³¼ì—ì„œ ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ\n",
        "        tool_match = re.search(\n",
        "            r'<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>',\n",
        "            completion,\n",
        "            re.DOTALL\n",
        "        )\n",
        "\n",
        "        if not tool_match:\n",
        "            # ë„êµ¬ í˜¸ì¶œì„ ì°¾ì§€ ëª»í•¨\n",
        "            rewards.append(-1.0)\n",
        "            continue\n",
        "\n",
        "        tool_json_str = tool_match.group(1)\n",
        "\n",
        "        # JSON íŒŒì‹± ì‹œë„\n",
        "        try:\n",
        "            tool_call = json.loads(tool_json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            # ì˜ëª»ëœ JSON - í•˜ì§€ë§Œ ìµœì†Œí•œ ë„êµ¬ í˜¸ì¶œì„ ì‹œë„í•¨\n",
        "            # í˜•ì‹ì´ ì˜ëª»ëœ ë¬¸ìì—´ì— ë„êµ¬ëª…ì´ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸\n",
        "            if expected_tool in tool_json_str.lower():\n",
        "                rewards.append(0.2)\n",
        "            else:\n",
        "                rewards.append(-0.5)\n",
        "            continue\n",
        "\n",
        "        # ìœ íš¨í•œ JSON - ì´ì œ ë„êµ¬ëª… í™•ì¸\n",
        "        tool_name = tool_call.get('name', '').lower().strip()\n",
        "\n",
        "        # ë„êµ¬ëª…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n",
        "        tool_name_correct = (\n",
        "            expected_tool in tool_name or\n",
        "            tool_name in expected_tool or\n",
        "            tool_name == expected_tool\n",
        "        )\n",
        "\n",
        "        if not tool_name_correct:\n",
        "            # ì˜ëª»ëœ ë„êµ¬ì´ì§€ë§Œ ìœ íš¨í•œ JSON\n",
        "            rewards.append(-0.3)\n",
        "            continue\n",
        "\n",
        "        # ì˜¬ë°”ë¥¸ ë„êµ¬ëª… + ìœ íš¨í•œ JSON\n",
        "        # ì œê³µëœ ê²½ìš° ë§¤ê°œë³€ìˆ˜ í™•ì¸\n",
        "        if expected_params and label_idx < len(expected_params):\n",
        "            required = expected_params[label_idx]\n",
        "            provided_params = tool_call.get('parameters', tool_call.get('arguments', {}))\n",
        "\n",
        "            if isinstance(provided_params, dict):\n",
        "                has_all_required = all(\n",
        "                    param in provided_params and provided_params[param] not in [None, '', []]\n",
        "                    for param in required\n",
        "                )\n",
        "\n",
        "                if has_all_required:\n",
        "                    rewards.append(1.0)  # ì™„ë²½!\n",
        "                else:\n",
        "                    rewards.append(0.5)  # ì˜¬ë°”ë¥¸ ë„êµ¬ì´ì§€ë§Œ ë§¤ê°œë³€ìˆ˜ ëˆ„ë½\n",
        "            else:\n",
        "                rewards.append(0.5)  # ì˜ëª»ëœ í˜•ì‹ì˜ ë§¤ê°œë³€ìˆ˜\n",
        "        else:\n",
        "            # ë§¤ê°œë³€ìˆ˜ ê²€ì‚¬ ì—†ìŒ - ë„êµ¬ëª…ë§Œ ê²€ì¦\n",
        "            rewards.append(1.0)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def reward_format_compliance(completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    í˜•ì‹ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ë³´ìƒ í•¨ìˆ˜.\n",
        "    ì˜¬ë°”ë¥¸ XML íƒœê·¸, JSON êµ¬ì¡° ë“±ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for completion in completions:\n",
        "        reward = 0.0\n",
        "\n",
        "        # ì˜¬ë°”ë¥¸ tool_call íƒœê·¸ í™•ì¸\n",
        "        if '<tool_call>' in completion and '</tool_call>' in completion:\n",
        "            reward += 0.3\n",
        "\n",
        "        # ê· í˜• ì¡íŒ ì¤‘ê´„í˜¸ í™•ì¸\n",
        "        if completion.count('{') == completion.count('}'):\n",
        "            reward += 0.2\n",
        "\n",
        "        # í‚¤ ì£¼ìœ„ì˜ ë”°ì˜´í‘œ í™•ì¸ (ê¸°ë³¸ JSON í˜•ì‹)\n",
        "        tool_match = re.search(r'<tool_call>(.*?)</tool_call>', completion, re.DOTALL)\n",
        "        if tool_match:\n",
        "            tool_content = tool_match.group(1).strip()\n",
        "            # ê¸°ë³¸ í™•ì¸: \"name\"ì´ ë”°ì˜´í‘œì™€ í•¨ê»˜ ìˆì–´ì•¼ í•¨\n",
        "            if '\"name\"' in tool_content or \"'name'\" in tool_content:\n",
        "                reward += 0.3\n",
        "\n",
        "            # íŒŒì‹± ê°€ëŠ¥í•œì§€ í™•ì¸\n",
        "            try:\n",
        "                json.loads(tool_content)\n",
        "                reward += 0.2  # ìœ íš¨í•œ JSONì— ëŒ€í•œ ë³´ë„ˆìŠ¤\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë³´ìƒ í•¨ìˆ˜ ê²°í•©\n",
        "def combined_reward(completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"ì—¬ëŸ¬ ë³´ìƒ ì‹ í˜¸ì˜ ê°€ì¤‘ì¹˜ ê²°í•©.\"\"\"\n",
        "    quality_rewards = reward_tool_call_quality(completions, **kwargs)\n",
        "    format_rewards = reward_format_compliance(completions, **kwargs)\n",
        "\n",
        "    # í˜•ì‹ë³´ë‹¤ í’ˆì§ˆì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
        "    combined = [\n",
        "        0.8 * q + 0.2 * f\n",
        "        for q, f in zip(quality_rewards, format_rewards)\n",
        "    ]\n",
        "\n",
        "    return combined\n",
        "\n",
        "\n",
        "# GRPO(Group Relative Policy Optimization) ì„¤ì •\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=\"ch07/fine_tuned_model/qwen-helpdesk-rlvr\",\n",
        "    num_generations=4,  # í”„ë¡¬í”„íŠ¸ë‹¹ ì—¬ëŸ¬ ì™„ì„± ê²°ê³¼ ìƒì„±\n",
        "    learning_rate=5e-6,\n",
        "    per_device_train_batch_size=4,  # num_generationsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "# GRPO(Group Relative Policy Optimization) íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
        "trainer = GRPOTrainer(\n",
        "    model=\"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    reward_funcs=combined_reward,\n",
        "    train_dataset=dataset,\n",
        "    args=grpo_config,\n",
        ")\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"í›ˆë ¨ ì¥ì¹˜: {trainer.model.device}\")\n",
        "\n",
        "# í›ˆë ¨ ì‹œì‘\n",
        "print(\"í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "trainer.train()\n",
        "\n",
        "print(f\"âœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: {grpo_config.output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}