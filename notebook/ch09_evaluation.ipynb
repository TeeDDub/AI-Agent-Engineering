{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 09: 평가 (Evaluation)\n",
        "\n",
        "이 노트북에서는 에이전트 메모리 평가 지표를 다룹니다.\n",
        "\n",
        "## 주요 내용\n",
        "- 정밀도/재현율/F1\n",
        "- 메모리 업데이트 평가\n",
        "- 메모리 검색 평가\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/Building-Applications-with-AI-Agents/blob/main/notebook/ch09_evaluation.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 패키지 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 필요한 외부 패키지가 없습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API 키 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✅ Colab Secrets에서 API 키를 불러왔습니다.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
        "    print(\"⚠️ API 키를 직접 입력해주세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. memory_evaluation.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "메모리 업데이트와 검색 평가 지표를 제공합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# memory_evaluation.py\n",
        "\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import statistics\n",
        "\n",
        "def precision_recall_f1(predicted: List[Any], expected: List[Any]) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    두 항목 리스트 간의 정밀도(precision), 재현율(recall), F1 점수를 계산합니다.\n",
        "    항목들은 동등성을 기준으로 비교됩니다. 순서는 중요하지 않습니다.\n",
        "    \"\"\"\n",
        "    pred_set = set(predicted)\n",
        "    exp_set = set(expected)\n",
        "    if not pred_set and not exp_set:\n",
        "        return 1.0, 1.0, 1.0\n",
        "    if not pred_set:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    tp = len(pred_set & exp_set)\n",
        "    precision = tp / len(pred_set)\n",
        "    recall = tp / len(exp_set) if exp_set else 1.0\n",
        "    if precision + recall == 0:\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def evaluate_memory_updates(\n",
        "    predicted_updates: List[Any],\n",
        "    expected_updates: List[Any]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    에이전트의 메모리 업데이트가 예상 업데이트와 얼마나 잘 일치하는지 평가합니다.\n",
        "    정밀도, 재현율, F1 점수를 반환합니다.\n",
        "    \"\"\"\n",
        "    p, r, f1 = precision_recall_f1(predicted_updates, expected_updates)\n",
        "    return {\"memory_precision\": p, \"memory_recall\": r, \"memory_f1\": f1}\n",
        "\n",
        "def evaluate_memory_retrieval(\n",
        "    retrieve_fn: Any,\n",
        "    queries: List[str],\n",
        "    expected_results: List[List[Any]],\n",
        "    top_k: int = 1\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    k개의 메모리 항목 리스트를 반환하는 검색 함수 `retrieve_fn(query, k)`가 주어졌을 때,\n",
        "    여러 쿼리에 대해 평가합니다.\n",
        "    반환값:\n",
        "      - `retrieval_accuracy@k`: 상위 k개 결과 중 적어도 하나의 예상 항목이 포함된 쿼리의 비율입니다.\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "    for query, expect in zip(queries, expected_results):\n",
        "        results = retrieve_fn(query, top_k)\n",
        "        # 예상 항목을 검색했는지 확인합니다.\n",
        "        if set(results) & set(expect):\n",
        "            hits += 1\n",
        "    accuracy = hits / len(queries) if queries else 1.0\n",
        "    return {f\"retrieval_accuracy@{top_k}\": accuracy}\n",
        "\n",
        "def aggregate_metrics(list_of_dicts: List[Dict[str, float]]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    지표 딕셔너리들의 리스트(예: evaluate_*의 출력)가 주어졌을 때,\n",
        "    각 지표의 평균을 계산합니다.\n",
        "    \"\"\"\n",
        "    if not list_of_dicts:\n",
        "        return {}\n",
        "    aggregated: Dict[str, float] = {}\n",
        "    keys = list_of_dicts[0].keys()\n",
        "    for k in keys:\n",
        "        vals = [d[k] for d in list_of_dicts if k in d]\n",
        "        aggregated[k] = statistics.mean(vals) if vals else 0.0\n",
        "    return aggregated\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}