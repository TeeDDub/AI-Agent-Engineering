{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 02: ê³ ê° ì§€ì› ì—ì´ì „íŠ¸\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì´ì»¤ë¨¸ìŠ¤ ê³ ê° ì§€ì› ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” ë‚´ìš©\n",
        "- LangGraph StateGraph ì‚¬ìš©ë²•\n",
        "- ë„êµ¬ ì •ì˜ ë° ë°”ì¸ë”©\n",
        "- ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì„±\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/AI-Agent-Engineering/blob/main/notebook/ch02_customer_support_agent.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langgraph openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API í‚¤ ì„¤ì •\n",
        "\n",
        "Google Colabì˜ Secrets ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ë°©ë²• 1: Colab Secrets ì‚¬ìš© (ê¶Œì¥)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"âœ… Colab Secretsì—ì„œ API í‚¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ë°©ë²• 2: ì§ì ‘ ì…ë ¥ (Secrets ì‚¬ìš© ë¶ˆê°€ ì‹œ)\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"  # ì—¬ê¸°ì— API í‚¤ ì…ë ¥\n",
        "    print(\"âš ï¸ API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í–ˆìŠµë‹ˆë‹¤. ë³´ì•ˆì— ì£¼ì˜í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ê³ ê° ì§€ì› ì—ì´ì „íŠ¸ êµ¬í˜„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# State íƒ€ì… ì •ì˜\n",
        "class AgentState(TypedDict):\n",
        "    order: dict\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "# ì£¼ë¬¸ ì·¨ì†Œ ë„êµ¬ ì •ì˜\n",
        "@tool\n",
        "def cancel_order(order_id: str) -> str:\n",
        "    \"\"\"ë°°ì†¡ë˜ì§€ ì•Šì€ ì£¼ë¬¸ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    # (ì—¬ê¸°ì„œ ì‹¤ì œ ë°±ì—”ë“œ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤)\n",
        "    return f\"ì£¼ë¬¸ {order_id}ì´(ê°€) ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—ì´ì „íŠ¸ êµ¬ì¡° ì •ì˜: LLM í˜¸ì¶œ, ë„êµ¬ ì‹¤í–‰, ë‹¤ì‹œ LLM í˜¸ì¶œ\n",
        "def call_model(state):\n",
        "    msgs = state[\"messages\"]\n",
        "    order = state.get(\"order\", {\"order_id\": \"UNKNOWN\"})\n",
        "\n",
        "    # LLM ì´ˆê¸°í™” (gpt-5-mini ì‚¬ìš©)\n",
        "    llm = init_chat_model(model=\"gpt-5-mini\", temperature=0)\n",
        "    llm_with_tools = llm.bind_tools([cancel_order])  # ë„êµ¬ ë°”ì¸ë”©\n",
        "\n",
        "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
        "    prompt = f'''ë‹¹ì‹ ì€ ì´ì»¤ë¨¸ìŠ¤ ì§€ì› ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\n",
        "        ì£¼ë¬¸ ID: {order['order_id']}\n",
        "        ê³ ê°ì´ ì·¨ì†Œë¥¼ ìš”ì²­í•˜ë©´ cancel_order(order_id)ë¥¼ í˜¸ì¶œí•˜ê³ \n",
        "        ê°„ë‹¨í•œ í™•ì¸ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„¸ìš”.\n",
        "        ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.'''\n",
        "    \n",
        "    full = [SystemMessage(content=prompt)] + msgs\n",
        "\n",
        "    # 1ì°¨ LLM íŒ¨ìŠ¤: ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ ê²°ì •\n",
        "    first = llm_with_tools.invoke(full)\n",
        "    out = [first]\n",
        "\n",
        "    if getattr(first, \"tool_calls\", None):\n",
        "        # cancel_order ë„êµ¬ ì‹¤í–‰\n",
        "        tc = first.tool_calls[0]\n",
        "        result = cancel_order.invoke(tc[\"args\"])\n",
        "        out.append(ToolMessage(content=result, tool_call_id=tc[\"id\"]))\n",
        "\n",
        "        # 2ì°¨ LLM íŒ¨ìŠ¤: ìµœì¢… í™•ì¸ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "        second = llm.invoke(full + out)\n",
        "        out.append(second)\n",
        "\n",
        "    return {\"messages\": out}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# StateGraphë¡œ ì—ì´ì „íŠ¸ êµ¬ì¡° ì—°ê²°\n",
        "def construct_graph():\n",
        "    g = StateGraph(AgentState)\n",
        "    g.add_node(\"assistant\", call_model)\n",
        "    g.set_entry_point(\"assistant\")\n",
        "    return g.compile()\n",
        "\n",
        "graph = construct_graph()\n",
        "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ê°€ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ì—ì´ì „íŠ¸ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰: ì£¼ë¬¸ ì·¨ì†Œ ìš”ì²­\n",
        "example_order = {\"order_id\": \"B73973\"}\n",
        "convo = [HumanMessage(content=\"ì£¼ë¬¸ #B73973ë¥¼ ì·¨ì†Œí•´ì£¼ì„¸ìš”.\")]\n",
        "\n",
        "result = graph.invoke({\"order\": example_order, \"messages\": convo})\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ¤– ê³ ê° ì§€ì› ì—ì´ì „íŠ¸ ì‘ë‹µ\")\n",
        "print(\"=\" * 50)\n",
        "for msg in result[\"messages\"]:\n",
        "    print(f\"\\n{msg.type}: {msg.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì¼ë°˜ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
        "general_question = [HumanMessage(content=\"ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?\")]\n",
        "result2 = graph.invoke({\"order\": example_order, \"messages\": general_question})\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ¤– ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ\")\n",
        "print(\"=\" * 50)\n",
        "for msg in result2[\"messages\"]:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
