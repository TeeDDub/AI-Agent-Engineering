{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 08: 공급망 에이전트 (Supply Chain Agent)\n",
        "\n",
        "이 노트북에서는 공급망/물류 도메인의 멀티 에이전트 설계를 다룹니다.\n",
        "\n",
        "## 주요 내용\n",
        "- 공급망 물류 에이전트\n",
        "- Actor-Critic 및 멀티 에이전트\n",
        "- ADAS/A2A 설계\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/Building-Applications-with-AI-Agents/blob/main/notebook/ch08_supply_chain_agent.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 패키지 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langgraph openai numpy pandas ray backoff tqdm requests python-dotenv traceloop-sdk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API 키 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✅ Colab Secrets에서 API 키를 불러왔습니다.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
        "    print(\"⚠️ API 키를 직접 입력해주세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. supply_chain_logistics_agent.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "공급망 물류 에이전트 워크플로우를 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\"\"\"\n",
        "supply_chain_logistics_agent.py\n",
        "재고 관리, 운송 작업, 공급업체 관계 및 창고 최적화를 처리하는 \n",
        "공급망 및 물류 관리 에이전트를 위한 LangGraph 워크플로우.\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import operator\n",
        "import builtins\n",
        "from typing import Annotated, Sequence, TypedDict, Optional\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from traceloop.sdk import Traceloop\n",
        "from src.common.observability.loki_logger import log_to_loki\n",
        "\n",
        "# 환경변수\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4317\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_INSECURE\"] = \"true\"\n",
        "\n",
        "@tool\n",
        "def manage_inventory(sku: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"재고 수준, 재고 보충, 감사 및 최적화 전략을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_inventory(sku={sku}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_inventory\", f\"sku={sku}\")\n",
        "    return \"inventory_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def track_shipments(origin: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 상태, 지연 사항을 추적하고 배송 물류를 조정합니다.\"\"\"\n",
        "    print(f\"[도구] track_shipments(origin={origin}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.track_shipments\", f\"origin={origin}\")\n",
        "    return \"shipment_tracking_updated\"\n",
        "\n",
        "@tool\n",
        "def evaluate_suppliers(supplier_name: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급업체 성과를 평가하고 감사를 수행하며 공급업체 관계를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] evaluate_suppliers(supplier_name={supplier_name}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.evaluate_suppliers\", f\"supplier_name={supplier_name}\")\n",
        "    return \"supplier_evaluation_complete\"\n",
        "\n",
        "@tool\n",
        "def optimize_warehouse(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"창고 운영, 레이아웃, 용량 및 보관 효율성을 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_warehouse(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_warehouse\", f\"operation_type={operation_type}\")\n",
        "    return \"warehouse_optimization_initiated\"\n",
        "\n",
        "@tool\n",
        "def forecast_demand(season: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"수요 패턴, 계절적 추세를 분석하고 예측 모델을 생성합니다.\"\"\"\n",
        "    print(f\"[도구] forecast_demand(season={season}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.forecast_demand\", f\"season={season}\")\n",
        "    return \"demand_forecast_generated\"\n",
        "\n",
        "@tool\n",
        "def manage_quality(supplier: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"품질 관리, 결함 추적 및 공급업체 품질 표준을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_quality(supplier={supplier}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_quality\", f\"supplier={supplier}\")\n",
        "    return \"quality_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def arrange_shipping(shipping_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 방법, 특급 배송 및 복합 운송을 준비합니다.\"\"\"\n",
        "    print(f\"[도구] arrange_shipping(shipping_type={shipping_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.arrange_shipping\", f\"shipping_type={shipping_type}\")\n",
        "    return \"shipping_arranged\"\n",
        "\n",
        "@tool\n",
        "def coordinate_operations(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"크로스도킹, 통합 및 이동과 같은 복잡한 작업을 조정합니다.\"\"\"\n",
        "    print(f\"[도구] coordinate_operations(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.coordinate_operations\", f\"operation_type={operation_type}\")\n",
        "    return \"operations_coordinated\"\n",
        "\n",
        "@tool\n",
        "def manage_special_handling(product_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"위험물, 콜드체인 및 민감한 제품에 대한 특수 요구사항을 처리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_special_handling(product_type={product_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_special_handling\", f\"product_type={product_type}\")\n",
        "    return \"special_handling_managed\"\n",
        "\n",
        "@tool\n",
        "def handle_compliance(compliance_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"규제 준수, 세관, 문서화 및 인증을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] handle_compliance(compliance_type={compliance_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.handle_compliance\", f\"compliance_type={compliance_type}\")\n",
        "    return \"compliance_handled\"\n",
        "\n",
        "@tool\n",
        "def process_returns(returned_quantity: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"반품, 역물류 및 제품 처리를 처리합니다.\"\"\"\n",
        "    print(f\"[도구] process_returns(returned_quantity={returned_quantity}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.process_returns\", f\"returned_quantity={returned_quantity}\")\n",
        "    return \"returns_processed\"\n",
        "\n",
        "@tool\n",
        "def scale_operations(scaling_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"성수기, 용량 계획 및 인력 관리를 위한 운영을 확장합니다.\"\"\"\n",
        "    print(f\"[도구] scale_operations(scaling_type={scaling_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.scale_operations\", f\"scaling_type={scaling_type}\")\n",
        "    return \"operations_scaled\"\n",
        "\n",
        "@tool\n",
        "def optimize_costs(cost_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"운송, 보관 및 운영 비용을 분석하고 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_costs(cost_type={cost_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_costs\", f\"cost_type={cost_type}\")\n",
        "    return \"cost_optimization_initiated\"\n",
        "\n",
        "@tool\n",
        "def optimize_delivery(delivery_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 경로, 라스트마일 물류 및 지속가능성 이니셔티브를 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_delivery(delivery_type={delivery_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_delivery\", f\"delivery_type={delivery_type}\")\n",
        "    return \"delivery_optimization_complete\"\n",
        "\n",
        "@tool\n",
        "def manage_disruption(disruption_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급망 중단, 비상 계획 및 위험 완화를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_disruption(disruption_type={disruption_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_disruption\", f\"disruption_type={disruption_type}\")\n",
        "    return \"disruption_managed\"\n",
        "\n",
        "@tool\n",
        "def send_logistics_response(operation_id: Optional[str] = None, message: Optional[str] = None) -> str:\n",
        "    \"\"\"이해관계자에게 물류 업데이트, 권장 사항 또는 상태 보고서를 전송합니다.\"\"\"\n",
        "    print(f\"[도구] send_logistics_response → {message}\")\n",
        "    log_to_loki(\"tool.send_logistics_response\", f\"operation_id={operation_id}, message={message}\")\n",
        "    return \"logistics_response_sent\"\n",
        "\n",
        "TOOLS = [\n",
        "    manage_inventory, track_shipments, evaluate_suppliers, optimize_warehouse,\n",
        "    forecast_demand, manage_quality, arrange_shipping, coordinate_operations,\n",
        "    manage_special_handling, handle_compliance, process_returns, scale_operations,\n",
        "    optimize_costs, optimize_delivery, manage_disruption, send_logistics_response\n",
        "]\n",
        "\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", callbacks=[StreamingStdOutCallbackHandler()],  \n",
        "    verbose=True).bind_tools(TOOLS)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    operation: Optional[dict]  # 공급망 운영 정보\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "def call_model(state: AgentState):\n",
        "    history = state[\"messages\"]\n",
        "    \n",
        "    # 누락되거나 불완전한 작업 데이터를 적절히 처리\n",
        "    operation = state.get(\"operation\", {})\n",
        "    if not operation:\n",
        "        operation = {\"operation_id\": \"UNKNOWN\", \"type\": \"general\", \"priority\": \"medium\", \"status\": \"active\"}\n",
        "    \n",
        "    operation_json = json.dumps(operation, ensure_ascii=False)\n",
        "    system_prompt = f\"\"\"\n",
        "        당신은 숙련된 공급망 및 물류 관리 전문가입니다.\n",
        "        전문 분야:\n",
        "        - 재고 관리 및 수요 예측\n",
        "        - 운송 및 배송 최적화\n",
        "        - 공급업체 관계 관리 및 평가\n",
        "        - 창고 운영 및 용량 계획\n",
        "        - 품질 관리 및 규정 준수 관리\n",
        "        - 비용 최적화 및 운영 효율성\n",
        "        - 위험 관리 및 중단 대응\n",
        "        - 지속가능성 및 친환경 물류 이니셔티브\n",
        "\n",
        "        공급망 운영을 관리할 때:\n",
        "        1) 물류 과제 또는 기회를 분석합니다\n",
        "        2) 적절한 공급망 관리 도구를 호출합니다\n",
        "        3) send_logistics_response로 권장 사항을 제공합니다\n",
        "        4) 비용, 효율성, 품질 및 지속가능성 영향을 고려합니다\n",
        "        5) 고객 만족도와 비즈니스 연속성을 우선시합니다\n",
        "\n",
        "        항상 비용 최적화와 서비스 품질 및 위험 완화의 균형을 유지하십시오.\n",
        "\n",
        "        작업: {operation_json}\"\"\"\n",
        "\n",
        "    full = [SystemMessage(content=system_prompt)] + history\n",
        "\n",
        "    first: ToolMessage | BaseMessage = llm.invoke(full)\n",
        "    messages = [first]\n",
        "\n",
        "    if getattr(first, \"tool_calls\", None):\n",
        "        for tc in first.tool_calls:\n",
        "            print(first)\n",
        "            print(tc['name'])\n",
        "            fn = next(t for t in TOOLS if t.name == tc['name'])\n",
        "            out = fn.invoke(tc[\"args\"])\n",
        "            messages.append(ToolMessage(content=str(out), tool_call_id=tc[\"id\"]))\n",
        "\n",
        "        second = llm.invoke(full + messages)\n",
        "        messages.append(second)\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "def construct_graph():\n",
        "    g = StateGraph(AgentState)\n",
        "    g.add_node(\"assistant\", call_model)\n",
        "    g.set_entry_point(\"assistant\")\n",
        "    return g.compile()\n",
        "\n",
        "graph = construct_graph()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Traceloop.init(disable_batch=True, app_name=\"supply_chain_logistics_agent_langgraph\")\n",
        "    example = {\"operation_id\": \"OP-12345\", \"type\": \"inventory_management\", \"priority\": \"high\", \"location\": \"Warehouse A\"}\n",
        "    convo = [HumanMessage(content=\"SKU-12345 재고가 심각하게 부족합니다. 현재 재고는 50개이지만 미처리 주문이 200개입니다. 재주문 전략은 무엇입니까?\")]\n",
        "    result = graph.invoke({\"operation\": example, \"messages\": convo})\n",
        "    for m in result[\"messages\"]:\n",
        "        print(f\"{m.type}: {m.content}\") ",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. supply_chain_logistics_actor_critic.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actor-Critic 기반 공급망 멀티 에이전트를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\"\"\"\n",
        "supply_chain_logistics_actor_critic.py\n",
        "Actor-Critic 패턴을 적용한 멀티 에이전트 공급망 및 물류 관리 시스템.\n",
        "Actor가 여러 후보 계획을 생성하고, Critic이 평가하여 최적의 계획을 선택하거나 재생성을 요청합니다.\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict, Optional\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from traceloop.sdk import Traceloop\n",
        "from src.common.observability.loki_logger import log_to_loki\n",
        "\n",
        "# 환경변수\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4317\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_INSECURE\"] = \"true\"\n",
        "\n",
        "# 모든 전문가를 위한 공유 도구\n",
        "@tool\n",
        "def send_logistics_response(operation_id: Optional[str] = None, message: Optional[str] = None) -> str:\n",
        "    \"\"\"이해관계자에게 물류 업데이트, 권장 사항 또는 상태 보고서를 전송합니다.\"\"\"\n",
        "    print(f\"[도구] send_logistics_response → {message}\")\n",
        "    log_to_loki(\"tool.send_logistics_response\", f\"operation_id={operation_id}, message={message}\")\n",
        "    return \"logistics_response_sent\"\n",
        "\n",
        "# 재고 및 창고 전문가 도구\n",
        "@tool\n",
        "def manage_inventory(sku: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"재고 수준, 재고 보충, 감사 및 최적화 전략을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_inventory(sku={sku}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_inventory\", f\"sku={sku}\")\n",
        "    return \"inventory_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def optimize_warehouse(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"창고 운영, 레이아웃, 용량 및 보관 효율성을 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_warehouse(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_warehouse\", f\"operation_type={operation_type}\")\n",
        "    return \"warehouse_optimization_initiated\"\n",
        "\n",
        "@tool\n",
        "def forecast_demand(season: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"수요 패턴, 계절적 추세를 분석하고 예측 모델을 생성합니다.\"\"\"\n",
        "    print(f\"[도구] forecast_demand(season={season}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.forecast_demand\", f\"season={season}\")\n",
        "    return \"demand_forecast_generated\"\n",
        "\n",
        "@tool\n",
        "def manage_quality(supplier: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"품질 관리, 결함 추적 및 공급업체 품질 표준을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_quality(supplier={supplier}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_quality\", f\"supplier={supplier}\")\n",
        "    return \"quality_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def scale_operations(scaling_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"성수기, 용량 계획 및 인력 관리를 위한 운영을 확장합니다.\"\"\"\n",
        "    print(f\"[도구] scale_operations(scaling_type={scaling_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.scale_operations\", f\"scaling_type={scaling_type}\")\n",
        "    return \"operations_scaled\"\n",
        "\n",
        "@tool\n",
        "def optimize_costs(cost_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"운송, 보관 및 운영 비용을 분석하고 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_costs(cost_type={cost_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_costs\", f\"cost_type={cost_type}\")\n",
        "    return \"cost_optimization_initiated\"\n",
        "\n",
        "INVENTORY_TOOLS = [manage_inventory, optimize_warehouse, forecast_demand, manage_quality, scale_operations, optimize_costs, send_logistics_response]\n",
        "\n",
        "# 운송 및 물류 전문가 도구\n",
        "@tool\n",
        "def track_shipments(origin: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 상태, 지연 사항을 추적하고 배송 물류를 조정합니다.\"\"\"\n",
        "    print(f\"[도구] track_shipments(origin={origin}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.track_shipments\", f\"origin={origin}\")\n",
        "    return \"shipment_tracking_updated\"\n",
        "\n",
        "@tool\n",
        "def arrange_shipping(shipping_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 방법, 특급 배송 및 복합 운송을 준비합니다.\"\"\"\n",
        "    print(f\"[도구] arrange_shipping(shipping_type={shipping_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.arrange_shipping\", f\"shipping_type={shipping_type}\")\n",
        "    return \"shipping_arranged\"\n",
        "\n",
        "@tool\n",
        "def coordinate_operations(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"크로스도킹, 통합 및 이동과 같은 복잡한 작업을 조정합니다.\"\"\"\n",
        "    print(f\"[도구] coordinate_operations(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.coordinate_operations\", f\"operation_type={operation_type}\")\n",
        "    return \"operations_coordinated\"\n",
        "\n",
        "@tool\n",
        "def manage_special_handling(product_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"위험물, 콜드체인 및 민감한 제품에 대한 특수 요구사항을 처리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_special_handling(product_type={product_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_special_handling\", f\"product_type={product_type}\")\n",
        "    return \"special_handling_managed\"\n",
        "\n",
        "@tool\n",
        "def process_returns(returned_quantity: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"반품, 역물류 및 제품 처리를 처리합니다.\"\"\"\n",
        "    print(f\"[도구] process_returns(returned_quantity={returned_quantity}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.process_returns\", f\"returned_quantity={returned_quantity}\")\n",
        "    return \"returns_processed\"\n",
        "\n",
        "@tool\n",
        "def optimize_delivery(delivery_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 경로, 라스트마일 물류 및 지속가능성 이니셔티브를 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_delivery(delivery_type={delivery_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_delivery\", f\"delivery_type={delivery_type}\")\n",
        "    return \"delivery_optimization_complete\"\n",
        "\n",
        "@tool\n",
        "def manage_disruption(disruption_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급망 중단, 비상 계획 및 위험 완화를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_disruption(disruption_type={disruption_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_disruption\", f\"disruption_type={disruption_type}\")\n",
        "    return \"disruption_managed\"\n",
        "\n",
        "TRANSPORTATION_TOOLS = [track_shipments, arrange_shipping, coordinate_operations, manage_special_handling, process_returns, optimize_delivery, manage_disruption, send_logistics_response]\n",
        "\n",
        "# 공급업체 및 규정 준수 전문가 도구\n",
        "@tool\n",
        "def evaluate_suppliers(supplier_name: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급업체 성과를 평가하고 감사를 수행하며 공급업체 관계를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] evaluate_suppliers(supplier_name={supplier_name}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.evaluate_suppliers\", f\"supplier_name={supplier_name}\")\n",
        "    return \"supplier_evaluation_complete\"\n",
        "\n",
        "@tool\n",
        "def handle_compliance(compliance_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"규제 준수, 세관, 문서화 및 인증을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] handle_compliance(compliance_type={compliance_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.handle_compliance\", f\"compliance_type={compliance_type}\")\n",
        "    return \"compliance_handled\"\n",
        "\n",
        "SUPPLIER_TOOLS = [evaluate_suppliers, handle_compliance, send_logistics_response]\n",
        "\n",
        "# 모든 도구 리스트\n",
        "ALL_TOOLS = INVENTORY_TOOLS + TRANSPORTATION_TOOLS + SUPPLIER_TOOLS\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", temperature=0.0, callbacks=[StreamingStdOutCallbackHandler()], verbose=True)\n",
        "\n",
        "# AgentState 정의 - candidates와 iteration 필드 추가\n",
        "class AgentState(TypedDict):\n",
        "    operation: Optional[dict]  # 공급망 운영 정보\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    candidates: Optional[list]  # Actor가 생성한 후보 계획들\n",
        "    iteration: Optional[int]  # 반복 횟수\n",
        "\n",
        "# Actor 노드: 후보 계획 생성\n",
        "def actor_node(state: AgentState):\n",
        "    \"\"\"3개의 후보 공급망 계획을 생성합니다.\"\"\"\n",
        "    history = state[\"messages\"]\n",
        "    actor_prompt = '''3개의 공급망 후보 계획을 JSON 리스트 형식으로 생성하세요.\n",
        "    형식: [{'plan': '계획 설명', 'tools': [{'tool': '도구명', 'args': {...}}]}]\n",
        "    각 계획은 실행 가능한 구체적인 단계와 필요한 도구를 포함해야 합니다.'''\n",
        "    response = llm.invoke([SystemMessage(content=actor_prompt)] + history)\n",
        "    try:\n",
        "        candidates = json.loads(response.content)\n",
        "    except json.JSONDecodeError:\n",
        "        # JSON 파싱 실패 시 기본 후보 제공\n",
        "        candidates = [{\"plan\": \"기본 계획\", \"tools\": []}]\n",
        "    return {\"candidates\": candidates, \"messages\": state[\"messages\"]}\n",
        "\n",
        "# Critic 노드: 평가 및 선택/반복\n",
        "def critic_node(state: AgentState):\n",
        "    \"\"\"후보 계획들을 평가하고 최적의 계획을 선택하거나 재생성을 요청합니다.\"\"\"\n",
        "    candidates = state.get(\"candidates\", [])\n",
        "    history = state[\"messages\"]\n",
        "    \n",
        "    critic_prompt = f'''다음 후보 계획들을 평가하세요: {candidates}\n",
        "    \n",
        "    실행 가능성(feasibility), 비용(cost), 위험도(risk) 기준으로 각각 1-10점으로 채점하세요.\n",
        "    \n",
        "    응답 형식 (JSON):\n",
        "    {{\n",
        "        \"evaluations\": [\n",
        "            {{\"plan_index\": 0, \"feasibility\": 점수, \"cost\": 점수, \"risk\": 점수, \"total\": 총점}},\n",
        "            ...\n",
        "        ],\n",
        "        \"best_score\": 최고점수,\n",
        "        \"selected\": 선택된_계획_객체,\n",
        "        \"feedback\": \"개선을 위한 피드백 (점수가 8점 이하인 경우)\"\n",
        "    }}\n",
        "    \n",
        "    최고 점수가 8점 이상이면 해당 계획을 선택하고, 그렇지 않으면 재생성을 요청하세요.'''\n",
        "    \n",
        "    response = llm.invoke([SystemMessage(content=critic_prompt)] + history)\n",
        "    \n",
        "    try:\n",
        "        evaluation = json.loads(response.content)\n",
        "    except json.JSONDecodeError:\n",
        "        # JSON 파싱 실패 시 첫 번째 후보 선택\n",
        "        evaluation = {\n",
        "            \"best_score\": 9,\n",
        "            \"selected\": candidates[0] if candidates else {\"plan\": \"기본 계획\", \"tools\": []},\n",
        "            \"feedback\": \"\"\n",
        "        }\n",
        "    \n",
        "    if evaluation.get('best_score', 0) > 8:\n",
        "        winning_plan = evaluation['selected']\n",
        "        # 선택된 계획의 도구들을 실행\n",
        "        messages = []\n",
        "        for tool_info in winning_plan.get('tools', []):\n",
        "            tool_name = tool_info.get('tool', '')\n",
        "            tool_args = tool_info.get('args', {})\n",
        "            tc = {'name': tool_name, 'args': tool_args, 'id': f'tool_{len(messages)}'}\n",
        "            \n",
        "            # 도구 찾기 및 실행\n",
        "            try:\n",
        "                fn = next(t for t in ALL_TOOLS if t.name == tool_name)\n",
        "                out = fn.invoke(tool_args)\n",
        "                messages.append(ToolMessage(content=str(out), tool_call_id=tc[\"id\"]))\n",
        "            except StopIteration:\n",
        "                print(f\"[경고] 도구를 찾을 수 없음: {tool_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[오류] 도구 실행 실패: {tool_name}, {e}\")\n",
        "        \n",
        "        # 최종 응답 전송\n",
        "        send_logistics_response.invoke({\"message\": winning_plan.get('plan', '계획 실행 완료')})\n",
        "        \n",
        "        final_message = AIMessage(\n",
        "            content=f\"선택된 계획: {winning_plan.get('plan', '')} (점수: {evaluation.get('best_score', 0)})\"\n",
        "        )\n",
        "        return {\"messages\": history + messages + [final_message]}\n",
        "    else:\n",
        "        # 반복: Actor에게 피드백 제공\n",
        "        feedback_message = AIMessage(\n",
        "            content=f\"재생성 필요: 개선 사항 - {evaluation.get('feedback', '더 나은 계획이 필요합니다.')}\"\n",
        "        )\n",
        "        return {\"messages\": history + [feedback_message]}\n",
        "\n",
        "# Actor-Critic 그래프 구성\n",
        "def construct_actor_critic_graph():\n",
        "    \"\"\"Actor-Critic 패턴을 사용한 공급망 관리 그래프를 구성합니다.\"\"\"\n",
        "    g = StateGraph(AgentState)\n",
        "    g.add_node(\"actor\", actor_node)\n",
        "    g.add_node(\"critic\", critic_node)\n",
        "    \n",
        "    g.set_entry_point(\"actor\")\n",
        "    g.add_edge(\"actor\", \"critic\")\n",
        "    \n",
        "    # 승인되지 않은 경우 다시 Actor로 돌아감 (조건부)\n",
        "    def should_continue(state: AgentState) -> str:\n",
        "        \"\"\"Critic이 재생성을 요청했는지 확인합니다.\"\"\"\n",
        "        if not state.get(\"messages\"):\n",
        "            return END\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        if hasattr(last_message, 'content') and \"재생성\" in last_message.content:\n",
        "            return \"actor\"\n",
        "        return END\n",
        "    \n",
        "    g.add_conditional_edges(\"critic\", should_continue)\n",
        "    \n",
        "    return g.compile()\n",
        "\n",
        "# 그래프 컴파일\n",
        "graph = construct_actor_critic_graph()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Traceloop.init(disable_batch=True, app_name=\"supply_chain_logistics_agent_langgraph\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"공급망 물류 Actor-Critic 시스템 시작\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 예제 작업\n",
        "    example_operation = {\n",
        "        \"operation_id\": \"OP-12345\",\n",
        "        \"type\": \"inventory_management\",\n",
        "        \"priority\": \"high\",\n",
        "        \"location\": \"Warehouse A\",\n",
        "        \"issue\": \"critical_shortage\"\n",
        "    }\n",
        "    \n",
        "    initial_message = HumanMessage(\n",
        "        content=\"SKU-12345 재고가 심각하게 부족합니다. 현재 재고는 50개이지만 미처리 주문이 200개입니다. \"\n",
        "                \"재주문 전략과 단기 해결책을 제시해주세요.\"\n",
        "    )\n",
        "    \n",
        "    # 그래프 실행\n",
        "    result = graph.invoke({\n",
        "        \"operation\": example_operation,\n",
        "        \"messages\": [initial_message],\n",
        "        \"candidates\": None,\n",
        "        \"iteration\": 0\n",
        "    })\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"최종 결과\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, m in enumerate(result[\"messages\"], 1):\n",
        "        msg_type = m.type if hasattr(m, 'type') else type(m).__name__\n",
        "        content = m.content if hasattr(m, 'content') else str(m)\n",
        "        print(f\"\\n[{i}] {msg_type}:\")\n",
        "        print(f\"  {content[:200]}{'...' if len(content) > 200 else ''}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실행 완료\")\n",
        "    print(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. automated_design_of_agentic_systems.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ADAS 프레임워크로 에이전트 시스템을 탐색합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import argparse\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from utils import random_id, bootstrap_confidence_interval\n",
        "from collections import namedtuple\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "import backoff\n",
        "import numpy as np\n",
        "import openai\n",
        "import pandas\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 환경변수\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# 작업에 맞는 prompt 가져오기\n",
        "# 새로운 작업을 위해서는 get_init_archive, get_prompt, get_reflexion_prompt 함수를 포함한 새로운 prompt 모듈을 생성하세요.\n",
        "\n",
        "Info = namedtuple('Info', ['name', 'author', 'content', 'iteration_idx'])\n",
        "\n",
        "FORMAT_INST = lambda request_keys: f\"\"\"Reply EXACTLY with the following JSON format.\\n{str(request_keys)}\\nDO NOT MISS ANY REQUEST FIELDS and ensure that your response is a well-formed JSON object!\\n\"\"\"\n",
        "ROLE_DESC = lambda role: f\"You are a {role}.\"\n",
        "SYSTEM_MSG = \"\"\n",
        "\n",
        "PRINT_LLM_DEBUG = False\n",
        "SEARCHING_MODE = True\n",
        "\n",
        "\n",
        "@backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
        "def get_json_response_from_gpt(\n",
        "        msg,\n",
        "        model,\n",
        "        system_message,\n",
        "        temperature=0.5\n",
        "):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": msg},\n",
        "        ],\n",
        "        temperature=temperature, stop=None, response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    json_dict = json.loads(content)\n",
        "    assert json_dict is not None\n",
        "    return json_dict\n",
        "\n",
        "\n",
        "@backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
        "def get_json_response_from_gpt_reflect(\n",
        "        msg_list,\n",
        "        model,\n",
        "        temperature=1\n",
        "):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=msg_list,\n",
        "            temperature=temperature, stop=None, response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        \n",
        "        # 디버깅: 응답 내용 확인\n",
        "        if PRINT_LLM_DEBUG:\n",
        "            print(f\"[DEBUG] Model: {model}\")\n",
        "            print(f\"[DEBUG] Response content: {content[:200] if content else 'EMPTY'}\")\n",
        "            print(f\"[DEBUG] Finish reason: {response.choices[0].finish_reason}\")\n",
        "            print(f\"[DEBUG] Usage: {response.usage}\")\n",
        "        \n",
        "        if not content:\n",
        "            raise ValueError(f\"Empty response from model {model}. Finish reason: {response.choices[0].finish_reason}\")\n",
        "        \n",
        "        json_dict = json.loads(content)\n",
        "        assert json_dict is not None\n",
        "        return json_dict\n",
        "    except Exception as e:\n",
        "        if PRINT_LLM_DEBUG:\n",
        "            print(f\"[DEBUG] Exception in get_json_response_from_gpt_reflect: {e}\")\n",
        "            print(f\"[DEBUG] Messages length: {len(msg_list)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "class LLMAgentBase:\n",
        "    \"\"\"\n",
        "    LLM 에이전트의 기본 클래스, 다양한 출력 형식을 위해 설정 가능합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_fields: list, agent_name: str,\n",
        "                 role='helpful assistant', model='gpt-5-mini', temperature=0.5) -> None:\n",
        "        self.output_fields = output_fields\n",
        "        self.agent_name = agent_name\n",
        "        self.role = role\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.id = random_id()  # Assume random_id from utils\n",
        "\n",
        "    def generate_prompt(self, input_infos, instruction, output_description) -> tuple:\n",
        "        output_fields_and_description = {key: output_description.get(key, f\"Your {key}.\") for key in self.output_fields}\n",
        "        system_prompt = ROLE_DESC(self.role) + \"\\n\\n\" + FORMAT_INST(output_fields_and_description)\n",
        "\n",
        "        input_infos_text = ''\n",
        "        for input_info in input_infos:\n",
        "            if isinstance(input_info, Info):\n",
        "                (field_name, author, content, iteration_idx) = input_info\n",
        "                if author == self.__repr__():\n",
        "                    author += ' (yourself)'\n",
        "                if field_name == 'task':\n",
        "                    input_infos_text += f'# Your Task:\\n{content}\\n\\n'\n",
        "                elif iteration_idx != -1:\n",
        "                    input_infos_text += f'### {field_name} #{iteration_idx + 1} by {author}:\\n{content}\\n\\n'\n",
        "                else:\n",
        "                    input_infos_text += f'### {field_name} by {author}:\\n{content}\\n\\n'\n",
        "\n",
        "        prompt = input_infos_text + instruction\n",
        "        return system_prompt, prompt\n",
        "\n",
        "    def query(self, input_infos: list, instruction, output_description, iteration_idx=-1) -> list:\n",
        "        system_prompt, prompt = self.generate_prompt(input_infos, instruction, output_description)\n",
        "        try:\n",
        "            response_json = get_json_response_from_gpt(prompt, self.model, system_prompt, self.temperature)\n",
        "            assert len(response_json) == len(self.output_fields), \"not returning enough fields\"\n",
        "        except Exception as e:\n",
        "            response_json = {key: '' for key in self.output_fields if key not in response_json}\n",
        "            for key in list(response_json):\n",
        "                if key not in self.output_fields:\n",
        "                    del response_json[key]\n",
        "        output_infos = [Info(key, self.__repr__(), value, iteration_idx) for key, value in response_json.items()]\n",
        "        return output_infos\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.agent_name} {self.id}\"\n",
        "\n",
        "    def __call__(self, input_infos: list, instruction, output_description, iteration_idx=-1):\n",
        "        return self.query(input_infos, instruction, output_description, iteration_idx)\n",
        "\n",
        "\n",
        "class AgentSystem:\n",
        "    \"\"\"\n",
        "    AgentSystem의 기본 클래스, ARC에서 피드백과 같은 작업별 동작을 확장할 수 있습니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, **task_specific_init):\n",
        "        for k, v in task_specific_init.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class BaseTask:\n",
        "    \"\"\"\n",
        "    작업의 추상 기본 클래스, 새로운 문제를 위해 서브클래스를 생성하세요.\n",
        "    Required methods:\n",
        "    - get_init_archive: 초기 솔루션.\n",
        "    - get_prompt: 새로운 솔루션을 생성하기 위한 프롬프트.\n",
        "    - get_reflexion_prompt: 반영을 위한 프롬프트.\n",
        "    - load_data: 검증/테스트 데이터 로드.\n",
        "    - format_task: 데이터를 프롬프트 문자열로 포맷.\n",
        "    - get_ground_truth: 데이터에서 진실 추출.\n",
        "    - evaluate_prediction: 예측과 진실 간의 점수 매기기 (예: 1/0 정확도).\n",
        "    - parse_prediction: 원시 전방 출력을 비교 가능한 형식으로 파싱.\n",
        "    - get_output_description: 프롬프트의 출력 필드를 딕셔너리로 가져오기.\n",
        "    - get_instruction: 프롬프트에 추가 지시사항.\n",
        "\n",
        "    Optional:\n",
        "    - prepare_task_queue: 병렬 평가를 위한 입력 준비 (기본: 간단한 리스트).\n",
        "    - get_agent_system: 사용자 정의 AgentSystem 인스턴스 (기본: 기본).\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_init_archive(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_prompt(self, archive):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_reflexion_prompt(self, prev_solution):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_data(self, mode):  # mode: True for search (val), False for eval (test)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def format_task(self, task_data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_ground_truth(self, task_data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def evaluate_prediction(self, prediction, ground_truth):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def parse_prediction(self, res):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_output_description(self):\n",
        "        return {}\n",
        "\n",
        "    def get_instruction(self):\n",
        "        return \"\"\n",
        "\n",
        "    def prepare_task_queue(self, data):\n",
        "        return [Info('task', 'User', self.format_task(d), -1) for d in data]\n",
        "\n",
        "    def get_agent_system(self, task_data=None):\n",
        "        return AgentSystem()\n",
        "\n",
        "\n",
        "# MMLU 예제 서브클래스 (이전 코드 기반)\n",
        "class MMLUTask(BaseTask):\n",
        "    def get_init_archive(self):\n",
        "        from mmlu_prompt import get_init_archive  # Task-specific\n",
        "        return get_init_archive()\n",
        "\n",
        "    def get_prompt(self, archive):\n",
        "        from mmlu_prompt import get_prompt\n",
        "        return get_prompt(archive)\n",
        "\n",
        "    def get_reflexion_prompt(self, prev_solution):\n",
        "        from mmlu_prompt import get_reflexion_prompt\n",
        "        return get_reflexion_prompt(prev_solution)\n",
        "\n",
        "    def load_data(self, mode):\n",
        "        df = pandas.read_csv(self.args.data_filename)\n",
        "        random.seed(self.args.shuffle_seed)\n",
        "        examples = [row.to_dict() for _, row in df.iterrows()]\n",
        "        random.shuffle(examples)\n",
        "        if mode:\n",
        "            return examples[:self.args.valid_size] * self.args.n_repeat\n",
        "        else:\n",
        "            start = self.args.valid_size\n",
        "            end = start + self.args.test_size\n",
        "            return examples[start:end] * self.args.n_repeat\n",
        "\n",
        "    def format_task(self, task_data):\n",
        "        from utils import format_multichoice_question  # Assume in utils\n",
        "        return format_multichoice_question(task_data)\n",
        "\n",
        "    def get_ground_truth(self, task_data):\n",
        "        LETTER_TO_INDEX = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
        "        return LETTER_TO_INDEX[task_data['Answer']]\n",
        "\n",
        "    def evaluate_prediction(self, prediction, ground_truth):\n",
        "        return 1 if prediction == ground_truth else 0\n",
        "\n",
        "    def parse_prediction(self, res):\n",
        "        LETTER_TO_INDEX = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
        "        try:\n",
        "            if isinstance(res, str) and res in LETTER_TO_INDEX:\n",
        "                return LETTER_TO_INDEX[res]\n",
        "            if 'A)' in str(res):\n",
        "                return 0\n",
        "            if 'B)' in str(res):\n",
        "                return 1\n",
        "            if 'C)' in str(res):\n",
        "                return 2\n",
        "            if 'D)' in str(res):\n",
        "                return 3\n",
        "            if isinstance(res, list) and len(res) > 1:\n",
        "                content = res[1].content\n",
        "                return LETTER_TO_INDEX[content] if content in LETTER_TO_INDEX else -1\n",
        "            if hasattr(res, 'content'):\n",
        "                content = res.content\n",
        "                if content in LETTER_TO_INDEX:\n",
        "                    return LETTER_TO_INDEX[content]\n",
        "                if 'A)' in content:\n",
        "                    return 0\n",
        "                if 'B)' in content:\n",
        "                    return 1\n",
        "                if 'C)' in content:\n",
        "                    return 2\n",
        "                if 'D)' in content:\n",
        "                    return 3\n",
        "        except:\n",
        "            pass\n",
        "        return -1\n",
        "\n",
        "    def get_output_description(self):\n",
        "        return {'answer': '당신의 답변. 알파벳 선택만 반환하세요, 즉 A 또는 B 또는 C 또는 D.'}\n",
        "\n",
        "\n",
        "# ARC 예제 서브클래스 (이전 코드 기반)\n",
        "class ARCTask(BaseTask):\n",
        "    def get_init_archive(self):\n",
        "        from arc_prompt import get_init_archive\n",
        "        return get_init_archive()\n",
        "\n",
        "    def get_prompt(self, archive):\n",
        "        from arc_prompt import get_prompt\n",
        "        return get_prompt(archive)\n",
        "\n",
        "    def get_reflexion_prompt(self, prev_solution):\n",
        "        from arc_prompt import get_reflexion_prompt\n",
        "        return get_reflexion_prompt(prev_solution)\n",
        "\n",
        "    def load_data(self, mode):\n",
        "        path = self.args.val_data_path if mode else self.args.test_data_path\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        return data * self.args.n_repeat\n",
        "\n",
        "    def format_task(self, task_data):\n",
        "        from utils import format_arc_data\n",
        "        task_str, _, _ = format_arc_data(task_data)\n",
        "        return task_str\n",
        "\n",
        "    def get_ground_truth(self, task_data):\n",
        "        return task_data['test'][0]['output']  # ARC 구조\n",
        "\n",
        "    def evaluate_prediction(self, prediction, ground_truth):\n",
        "        from utils import eval_solution\n",
        "        arc_data = {'test': [{'output': ground_truth}]}\n",
        "        return eval_solution(prediction, arc_data, soft_eval=False)\n",
        "\n",
        "    def parse_prediction(self, res):\n",
        "        try:\n",
        "            if isinstance(res, Info):\n",
        "                res = res.content\n",
        "            if isinstance(res, str):\n",
        "                res = eval(res)\n",
        "            return res\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def get_output_description(self):\n",
        "        return {'answer': '당신의 답변. list[list[int]] 형식의 문자열만 반환하세요. 다른 것은 반환하지 마세요.'}\n",
        "\n",
        "    def get_instruction(self):\n",
        "        return \"`transform` 함수를 생성하여 이 작업을 해결하세요. 이 함수는 단일 인수를 받아들이며, 입력 그리드를 `list[list[int]]` 형식으로 받아들이고, 변환된 그리드를 `list[list[int]]` 형식으로 반환해야 합니다.\"\n",
        "\n",
        "    def prepare_task_queue(self, data):\n",
        "        from utils import format_arc_data\n",
        "        queue = []\n",
        "        for arc_data in data:\n",
        "            task_str, examples, test_input = format_arc_data(arc_data)\n",
        "            taskInfo = Info('task', 'User', task_str, -1)\n",
        "            agent = self.get_agent_system(examples=examples, test_input=test_input)\n",
        "            queue.append((agent, taskInfo, arc_data))\n",
        "        return queue\n",
        "\n",
        "    def get_agent_system(self, **kwargs):\n",
        "        from utils import list_to_string  # Assume in utils\n",
        "        class ARCAgentSystem(AgentSystem):\n",
        "            def __init__(self, examples, test_input):\n",
        "                super().__init__(examples=examples, test_input=test_input)\n",
        "\n",
        "            # run_examples_and_get_feedback 및 get_test_output_from_code가 필요한 경우 추가\n",
        "            # (omitted for brevity, but can copy from original ARC code)\n",
        "\n",
        "        return ARCAgentSystem(kwargs['examples'], kwargs['test_input'])\n",
        "\n",
        "\n",
        "def search(args, task):\n",
        "    file_path = os.path.join(args.save_dir, f\"{args.expr_name}_run_archive.json\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            archive = json.load(f)\n",
        "        last_gen = archive[-1].get('generation', 0) if archive else 0\n",
        "        start = int(last_gen) if isinstance(last_gen, int) else 0\n",
        "    else:\n",
        "        archive = task.get_init_archive()\n",
        "        start = 0\n",
        "\n",
        "    # 초기 아카이브 평가\n",
        "    for solution in archive:\n",
        "        if 'fitness' in solution:\n",
        "            continue\n",
        "        solution['generation'] = \"initial\"\n",
        "        print(f\"============Initial Archive: {solution.get('name', 'unnamed')}=================\")\n",
        "        try:\n",
        "            acc_list = evaluate_forward_fn(args, solution[\"code\"], task)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating initial: {e}\")\n",
        "            continue\n",
        "        solution['fitness'] = bootstrap_confidence_interval(acc_list)  \n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(archive, f, indent=4)\n",
        "\n",
        "    for n in range(start, args.n_generation):\n",
        "        print(f\"============Generation {n + 1}=================\")\n",
        "        system_prompt, prompt = task.get_prompt(archive)\n",
        "        msg_list = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
        "        try:\n",
        "            next_solution = get_json_response_from_gpt_reflect(msg_list, args.model)\n",
        "            ref1, ref2 = task.get_reflexion_prompt(archive[-1] if n > 0 else None)\n",
        "            msg_list += [{\"role\": \"assistant\", \"content\": str(next_solution)}, {\"role\": \"user\", \"content\": ref1}]\n",
        "            next_solution = get_json_response_from_gpt_reflect(msg_list, args.model)\n",
        "            msg_list += [{\"role\": \"assistant\", \"content\": str(next_solution)}, {\"role\": \"user\", \"content\": ref2}]\n",
        "            next_solution = get_json_response_from_gpt_reflect(msg_list, args.model)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating solution: {e}\")\n",
        "            continue\n",
        "\n",
        "        acc_list = []\n",
        "        for _ in range(args.debug_max):\n",
        "            try:\n",
        "                acc_list = evaluate_forward_fn(args, next_solution[\"code\"], task)\n",
        "                if np.mean(acc_list) < 0.01 and SEARCHING_MODE:\n",
        "                    raise Exception(\"All 0 accuracy\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation error: {e}\")\n",
        "                msg_list += [{\"role\": \"assistant\", \"content\": str(next_solution)}, \n",
        "                             {\"role\": \"user\", \"content\": f\"Error: {e}\\nDebug and repeat thought in 'thought', debug in 'debug_thought'\"}]\n",
        "                try:\n",
        "                    next_solution = get_json_response_from_gpt_reflect(msg_list, args.model)\n",
        "                except Exception as ee:\n",
        "                    print(f\"Error debugging: {ee}\")\n",
        "                    break\n",
        "        if not acc_list:\n",
        "            continue\n",
        "\n",
        "        next_solution['fitness'] = bootstrap_confidence_interval(acc_list)\n",
        "        next_solution['generation'] = n + 1\n",
        "        next_solution.pop('debug_thought', None)\n",
        "        next_solution.pop('reflection', None)\n",
        "        archive.append(next_solution)\n",
        "\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(archive, f, indent=4)\n",
        "\n",
        "\n",
        "def evaluate(args, task):\n",
        "    file_path = os.path.join(args.save_dir, f\"{args.expr_name}_run_archive.json\")\n",
        "    eval_file_path = file_path.replace(\".json\", \"_evaluate.json\")\n",
        "    with open(file_path, 'r') as f:\n",
        "        archive = json.load(f)\n",
        "    eval_archive = json.load(open(eval_file_path)) if os.path.exists(eval_file_path) else []\n",
        "\n",
        "    current_idx = len(eval_archive)\n",
        "    while current_idx < len(archive):\n",
        "        sol = archive[current_idx]\n",
        "        print(f\"Evaluating gen {sol['generation']}, idx {current_idx}\")\n",
        "        try:\n",
        "            acc_list = evaluate_forward_fn(args, sol[\"code\"], task)\n",
        "            sol['test_fitness'] = bootstrap_confidence_interval(acc_list)\n",
        "            eval_archive.append(sol)\n",
        "            with open(eval_file_path, 'w') as f:\n",
        "                json.dump(eval_archive, f, indent=4)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "        current_idx += 1\n",
        "\n",
        "\n",
        "def evaluate_forward_fn(args, forward_str, task):\n",
        "    namespace = {}\n",
        "    exec(forward_str, globals(), namespace)\n",
        "    func = list(namespace.values())[0]\n",
        "    if not callable(func):\n",
        "        raise ValueError(\"Not callable\")\n",
        "    AgentSystem.forward = staticmethod(func)  # 클래스에 첨부\n",
        "\n",
        "    data = task.load_data(SEARCHING_MODE)\n",
        "    task_queue = task.prepare_task_queue(data)\n",
        "    max_workers = min(len(task_queue), args.max_workers) if args.multiprocessing else 1\n",
        "\n",
        "    def process_item(item):\n",
        "        # ARC task는 (agent, taskInfo, task_data) tuple을 반환하고\n",
        "        # MMLU task는 Info 객체만 반환합니다.\n",
        "        # Info는 namedtuple이므로 isinstance(item, tuple)이 True지만\n",
        "        # 길이가 3이 아니라 4입니다.\n",
        "        if isinstance(item, tuple) and not isinstance(item, Info):\n",
        "            agent, taskInfo, task_data = item\n",
        "        else:\n",
        "            agent = task.get_agent_system()\n",
        "            taskInfo = item\n",
        "            # 해당 데이터 찾기 (인덱스 필요 시)\n",
        "            idx = task_queue.index(item)\n",
        "            task_data = data[idx]\n",
        "        res = agent.forward(taskInfo)\n",
        "        prediction = task.parse_prediction(res)\n",
        "        ground_truth = task.get_ground_truth(task_data)\n",
        "        if prediction is None:\n",
        "            return 0\n",
        "        return task.evaluate_prediction(prediction, ground_truth)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers) as executor:\n",
        "        acc_list = list(tqdm(executor.map(process_item, task_queue), total=len(task_queue)))\n",
        "\n",
        "    print(f\"acc: {bootstrap_confidence_interval(acc_list)}\")  \n",
        "    return acc_list\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Generic ADAS Framework\")\n",
        "    parser.add_argument('--task', type=str, required=True, choices=['mmlu', 'arc'], help=\"Task type\")\n",
        "    # Common\n",
        "    parser.add_argument('--save_dir', type=str, default='results/')\n",
        "    parser.add_argument('--expr_name', type=str, default='adas')\n",
        "    parser.add_argument('--n_generation', type=int, default=30)\n",
        "    parser.add_argument('--debug_max', type=int, default=3)\n",
        "    parser.add_argument('--model', type=str, default='gpt-5-mini')\n",
        "    parser.add_argument('--n_repeat', type=int, default=1)\n",
        "    parser.add_argument('--multiprocessing', action='store_true')\n",
        "    parser.add_argument('--max_workers', type=int, default=48)\n",
        "    # MMLU-specific\n",
        "    parser.add_argument('--data_filename', type=str)\n",
        "    parser.add_argument('--valid_size', type=int, default=128)\n",
        "    parser.add_argument('--test_size', type=int, default=800)\n",
        "    parser.add_argument('--shuffle_seed', type=int, default=0)\n",
        "    # ARC-specific\n",
        "    parser.add_argument('--val_data_path', type=str)\n",
        "    parser.add_argument('--test_data_path', type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.task == 'mmlu':\n",
        "        task = MMLUTask(args)\n",
        "        args.expr_name += '_mmlu'\n",
        "    elif args.task == 'arc':\n",
        "        if not args.val_data_path or not args.test_data_path:\n",
        "            print(\"오류: ARC 태스크는 --val_data_path와 --test_data_path 인자가 필요합니다.\")\n",
        "            exit(1)\n",
        "        task = ARCTask(args)\n",
        "        args.expr_name += '_arc'\n",
        "    else:\n",
        "        print(\"Error: Invalid task type. Please choose 'mmlu' or 'arc'.\")\n",
        "        exit(1)\n",
        "\n",
        "    SEARCHING_MODE = True\n",
        "    search(args, task)",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. agent_server.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 코드는 HTTP 서버를 실행합니다. Colab에서는 서버 실행이 제한될 수 있으니 원본 파일을 참고해 로컬에서 실행하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A2A 스펙 기반 에이전트 서버를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from openai import OpenAI\n",
        "\n",
        "# 환경변수 로드\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# 에이전트 카드 (발견을 위한 JSON 설명자)\n",
        "agent_card = {\n",
        "    \"name\": \"SummarizerAgent\",\n",
        "    \"description\": \"텍스트 요약을 수행하는 AI 에이전트입니다.\",\n",
        "    \"protocolVersion\": \"1.0\",\n",
        "    \"url\": \"http://localhost:8000\",\n",
        "    \"provider\": {\n",
        "        \"organization\": \"Example Org\",\n",
        "        \"url\": \"https://example.org\"\n",
        "    },\n",
        "    \"capabilities\": {\n",
        "        \"streaming\": False,\n",
        "        \"pushNotifications\": False,\n",
        "        \"stateTransitionHistory\": False\n",
        "    },\n",
        "    \"skills\": [\n",
        "        {\n",
        "            \"id\": \"summarize-text\",\n",
        "            \"name\": \"텍스트 요약\",\n",
        "            \"description\": \"주어진 텍스트를 간결하게 요약합니다.\",\n",
        "            \"tags\": [\"summarization\", \"nlp\", \"text-processing\"],\n",
        "            \"examples\": [\n",
        "                \"이 기사를 요약해 주세요\",\n",
        "                \"다음 내용을 간단히 정리해 주세요\"\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"defaultInputModes\": [\"text/plain\"],\n",
        "    \"defaultOutputModes\": [\"text/plain\"],\n",
        "    \"security\": []  # 프로덕션에서는 OAuth2, API 키 등을 설정하세요\n",
        "}\n",
        "\n",
        "\n",
        "class AgentHandler(BaseHTTPRequestHandler):\n",
        "    def do_GET(self):\n",
        "        # A2A 스펙: /.well-known/agent-card.json (섹션 8.2, 14.3)\n",
        "        if self.path == '/.well-known/agent-card.json':\n",
        "            self.send_response(200)\n",
        "            self.send_header('Content-type', 'application/json; charset=utf-8')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps(agent_card, ensure_ascii=False).encode('utf-8'))\n",
        "        else:\n",
        "            self.send_response(404)\n",
        "            self.end_headers()\n",
        "\n",
        "    def do_POST(self):\n",
        "        if self.path == '/':\n",
        "            content_length = int(self.headers['Content-Length'])\n",
        "            post_data = self.rfile.read(content_length)\n",
        "            rpc_request = json.loads(post_data)\n",
        "            \n",
        "            # A2A JSON-RPC 요청 처리 (섹션 9.4.1 message/send)\n",
        "            if rpc_request.get('jsonrpc') == '2.0' and rpc_request['method'] == 'message/send':\n",
        "                params = rpc_request.get('params', {})\n",
        "                message = params.get('message', {})\n",
        "                parts = message.get('parts', [])\n",
        "                \n",
        "                # 텍스트 파트 추출 (섹션 4.1.6 Part)\n",
        "                text = \"\"\n",
        "                for part in parts:\n",
        "                    if 'text' in part:\n",
        "                        text += part['text']\n",
        "                \n",
        "                # OpenAI API를 사용한 실제 LLM 요약\n",
        "                client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "                try:\n",
        "                    llm_response = client.chat.completions.create(\n",
        "                        model=\"gpt-4o-mini\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"당신은 간결한 요약을 제공하는 유용한 어시스턴트입니다.\"},\n",
        "                            {\"role\": \"user\", \"content\": f\"다음 텍스트를 요약하세요:\\n{text}\"}\n",
        "                        ],\n",
        "                    )\n",
        "                    summary = llm_response.choices[0].message.content.strip()\n",
        "                except Exception as e:\n",
        "                    summary = f\"요약 중 오류 발생: {str(e)}\"\n",
        "                \n",
        "                # A2A 스펙 준수 응답 (섹션 4.1.1 Task, 4.1.2 TaskStatus)\n",
        "                task_id = str(uuid.uuid4())\n",
        "                response = {\n",
        "                    \"jsonrpc\": \"2.0\",\n",
        "                    \"result\": {\n",
        "                        \"id\": task_id,\n",
        "                        \"contextId\": params.get('contextId', str(uuid.uuid4())),\n",
        "                        \"status\": {\n",
        "                            \"state\": \"completed\"\n",
        "                        },\n",
        "                        \"artifacts\": [\n",
        "                            {\n",
        "                                \"parts\": [{\"text\": summary}]\n",
        "                            }\n",
        "                        ]\n",
        "                    },\n",
        "                    \"id\": rpc_request['id']\n",
        "                }\n",
        "                self.send_response(200)\n",
        "                self.send_header('Content-type', 'application/json; charset=utf-8')\n",
        "                self.end_headers()\n",
        "                self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))\n",
        "            else:\n",
        "                # JSON-RPC 오류 처리 (섹션 9.5)\n",
        "                error_response = {\n",
        "                    \"jsonrpc\": \"2.0\",\n",
        "                    \"error\": {\"code\": -32601, \"message\": \"Method not found\"},\n",
        "                    \"id\": rpc_request.get('id')\n",
        "                }\n",
        "                self.send_response(200)  # JSON-RPC 에러도 HTTP 200으로 반환\n",
        "                self.send_header('Content-type', 'application/json; charset=utf-8')\n",
        "                self.end_headers()\n",
        "                self.wfile.write(json.dumps(error_response, ensure_ascii=False).encode('utf-8'))\n",
        "        else:\n",
        "            self.send_response(404)\n",
        "            self.end_headers()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    server_address = ('', 8000)\n",
        "    httpd = HTTPServer(server_address, AgentHandler)\n",
        "    print(\"A2A 에이전트 서버를 시작합니다. 주소: http://localhost:8000\")\n",
        "    print(\"Agent Card: http://localhost:8000/.well-known/agent-card.json\")\n",
        "    httpd.serve_forever()",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. agent_client.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 코드는 로컬에서 실행 중인 A2A 서버에 연결합니다. Colab에서는 로컬 서버에 접속하기 어려우니 원본 파일을 참고해 로컬에서 실행하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A2A 에이전트 호출 클라이언트를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "# 단계 1: 에이전트 검색 - A2A 스펙 준수 well-known URI\n",
        "card_url = 'http://localhost:8000/.well-known/agent-card.json'\n",
        "response = requests.get(card_url)\n",
        "if response.status_code != 200:\n",
        "    print(\"에이전트 카드 가져오기 실패\")\n",
        "    exit()\n",
        "\n",
        "agent_card = response.json()\n",
        "print(\"발견된 에이전트 카드:\", json.dumps(agent_card, indent=2, ensure_ascii=False))\n",
        "\n",
        "# 단계 2: 핸드셰이크 (버전 및 기능 확인)\n",
        "if agent_card.get('protocolVersion', '').split('.')[0] != '1':\n",
        "    print(\"호환되지 않는 프로토콜 버전\")\n",
        "    exit()\n",
        "\n",
        "# skills 확인\n",
        "skills = agent_card.get('skills', [])\n",
        "skill_ids = [s.get('id') for s in skills]\n",
        "if \"summarize-text\" not in skill_ids:\n",
        "    print(\"필요한 스킬이 지원되지 않음\")\n",
        "    exit()\n",
        "print(\"핸드셰이크 성공: 에이전트가 호환됩니다.\")\n",
        "\n",
        "# 단계 3: A2A 스펙 준수 JSON-RPC 요청 (message/send)\n",
        "rpc_url = agent_card['url']  # 에이전트 기본 URL로 POST\n",
        "rpc_request = {\n",
        "    \"jsonrpc\": \"2.0\",\n",
        "    \"method\": \"message/send\",\n",
        "    \"params\": {\n",
        "        \"contextId\": str(uuid.uuid4()),\n",
        "        \"message\": {\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [\n",
        "                {\n",
        "                    \"text\": \"이것은 요약이 필요한 긴 예제 텍스트입니다. 멀티 에이전트 시스템, 통신 프로토콜, A2A와 같은 표준을 사용하여 에이전트들이 자율적으로 협업하는 방법을 논의합니다.\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    \"id\": 123  # 고유한 요청 ID\n",
        "}\n",
        "\n",
        "response = requests.post(rpc_url, json=rpc_request)\n",
        "if response.status_code == 200:\n",
        "    rpc_response = response.json()\n",
        "    print(\"\\nRPC 응답:\", json.dumps(rpc_response, indent=2, ensure_ascii=False))\n",
        "    \n",
        "    # 결과 파싱\n",
        "    if 'result' in rpc_response:\n",
        "        result = rpc_response['result']\n",
        "        print(f\"\\n태스크 ID: {result.get('id')}\")\n",
        "        print(f\"상태: {result.get('status', {}).get('state')}\")\n",
        "        \n",
        "        artifacts = result.get('artifacts', [])\n",
        "        if artifacts:\n",
        "            for artifact in artifacts:\n",
        "                for part in artifact.get('parts', []):\n",
        "                    if 'text' in part:\n",
        "                        print(f\"\\n요약 결과:\\n{part['text']}\")\n",
        "else:\n",
        "    print(\"오류:\", response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ray_supply_chain_multi_agent.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 코드는 Ray 클러스터 환경이 필요합니다. Colab에서는 실행이 제한될 수 있으니 원본 파일을 참고해 로컬에서 실행하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ray 기반 공급망 멀티 에이전트 예제를 구성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\"\"\"\n",
        "supply_chain_logistics_agent_ray_per_session.py\n",
        "세션별 격리를 통한 Ray 액터를 사용하는 멀티 에이전트 공급망 및 물류 관리 시스템을 위한 LangGraph 워크플로우.\n",
        "Ray 액터로 구성된 전문 에이전트를 통해 재고 관리, 운송 작업, 공급업체 관계 및 창고 최적화를 처리합니다.\n",
        "각 세션(operation_id로 식별)은 전문가 유형별로 자체 액터 인스턴스를 가지며, 세션별 격리된 상태와 순차 실행을 보장합니다.\n",
        "슈퍼바이저는 전문가를 결정하고 SessionManager를 통해 세션별 액터를 원격으로 호출합니다.\n",
        "실행법: 파이썬 3.12를 기준으로 생성한 venv에서 pip로 ray를 설치하고, python ch08/ray_supply_chain_multi_agent.py 명령어로 실행합니다.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Annotated, Sequence, TypedDict, Optional, Dict\n",
        "\n",
        "import ray\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from traceloop.sdk import Traceloop\n",
        "from src.common.observability.loki_logger import log_to_loki\n",
        "\n",
        "# 환경변수 확인\n",
        "import os\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  \n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY가 설정되지 않았습니다.\"\n",
        "        \"환경변수 또는 .env 파일에서 설정해주세요.\"\n",
        "    )\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4317\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_INSECURE\"] = \"true\"\n",
        "\n",
        "# 모든 전문가를 위한 공유 도구\n",
        "@tool\n",
        "def send_logistics_response(operation_id: Optional[str] = None, message: Optional[str] = None) -> str:\n",
        "    \"\"\"이해관계자에게 물류 업데이트, 권장 사항 또는 상태 보고서를 전송합니다.\"\"\"\n",
        "    print(f\"[도구] send_logistics_response → {message}\")\n",
        "    log_to_loki(\"tool.send_logistics_response\", f\"operation_id={operation_id}, message={message}\")\n",
        "    return \"logistics_response_sent\"\n",
        "\n",
        "# 재고 및 창고 전문가 도구\n",
        "@tool\n",
        "def manage_inventory(sku: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"재고 수준, 재고 보충, 감사 및 최적화 전략을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_inventory(sku={sku}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_inventory\", f\"sku={sku}\")\n",
        "    return \"inventory_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def optimize_warehouse(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"창고 운영, 레이아웃, 용량 및 보관 효율성을 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_warehouse(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_warehouse\", f\"operation_type={operation_type}\")\n",
        "    return \"warehouse_optimization_initiated\"\n",
        "\n",
        "@tool\n",
        "def forecast_demand(season: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"수요 패턴, 계절적 추세를 분석하고 예측 모델을 생성합니다.\"\"\"\n",
        "    print(f\"[도구] forecast_demand(season={season}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.forecast_demand\", f\"season={season}\")\n",
        "    return \"demand_forecast_generated\"\n",
        "\n",
        "@tool\n",
        "def manage_quality(supplier: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"품질 관리, 결함 추적 및 공급업체 품질 표준을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_quality(supplier={supplier}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_quality\", f\"supplier={supplier}\")\n",
        "    return \"quality_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def scale_operations(scaling_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"성수기, 용량 계획 및 인력 관리를 위한 운영을 확장합니다.\"\"\"\n",
        "    print(f\"[도구] scale_operations(scaling_type={scaling_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.scale_operations\", f\"scaling_type={scaling_type}\")\n",
        "    return \"operations_scaled\"\n",
        "\n",
        "@tool\n",
        "def optimize_costs(cost_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"운송, 보관 및 운영 비용을 분석하고 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_costs(cost_type={cost_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_costs\", f\"cost_type={cost_type}\")\n",
        "    return \"cost_optimization_initiated\"\n",
        "\n",
        "INVENTORY_TOOLS = [manage_inventory, optimize_warehouse, forecast_demand, manage_quality, scale_operations, optimize_costs, send_logistics_response]\n",
        "\n",
        "# 운송 및 물류 전문가 도구\n",
        "@tool\n",
        "def track_shipments(origin: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 상태, 지연 사항을 추적하고 배송 물류를 조정합니다.\"\"\"\n",
        "    print(f\"[도구] track_shipments(origin={origin}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.track_shipments\", f\"origin={origin}\")\n",
        "    return \"shipment_tracking_updated\"\n",
        "\n",
        "@tool\n",
        "def arrange_shipping(shipping_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 방법, 특급 배송 및 복합 운송을 준비합니다.\"\"\"\n",
        "    print(f\"[도구] arrange_shipping(shipping_type={shipping_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.arrange_shipping\", f\"shipping_type={shipping_type}\")\n",
        "    return \"shipping_arranged\"\n",
        "\n",
        "@tool\n",
        "def coordinate_operations(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"크로스도킹, 통합 및 이동과 같은 복잡한 작업을 조정합니다.\"\"\"\n",
        "    print(f\"[도구] coordinate_operations(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.coordinate_operations\", f\"operation_type={operation_type}\")\n",
        "    return \"operations_coordinated\"\n",
        "\n",
        "@tool\n",
        "def manage_special_handling(product_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"위험물, 콜드체인 및 민감한 제품에 대한 특수 요구사항을 처리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_special_handling(product_type={product_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_special_handling\", f\"product_type={product_type}\")\n",
        "    return \"special_handling_managed\"\n",
        "\n",
        "@tool\n",
        "def process_returns(returned_quantity: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"반품, 역물류 및 제품 처리를 처리합니다.\"\"\"\n",
        "    print(f\"[도구] process_returns(returned_quantity={returned_quantity}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.process_returns\", f\"returned_quantity={returned_quantity}\")\n",
        "    return \"returns_processed\"\n",
        "\n",
        "@tool\n",
        "def optimize_delivery(delivery_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 경로, 라스트마일 물류 및 지속가능성 이니셔티브를 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_delivery(delivery_type={delivery_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_delivery\", f\"delivery_type={delivery_type}\")\n",
        "    return \"delivery_optimization_complete\"\n",
        "\n",
        "@tool\n",
        "def manage_disruption(disruption_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급망 중단, 비상 계획 및 위험 완화를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_disruption(disruption_type={disruption_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_disruption\", f\"disruption_type={disruption_type}\")\n",
        "    return \"disruption_managed\"\n",
        "\n",
        "TRANSPORTATION_TOOLS = [track_shipments, arrange_shipping, coordinate_operations, manage_special_handling, process_returns, optimize_delivery, manage_disruption, send_logistics_response]\n",
        "\n",
        "# 공급업체 및 규정 준수 전문가 도구\n",
        "@tool\n",
        "def evaluate_suppliers(supplier_name: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급업체 성과를 평가하고 감사를 수행하며 공급업체 관계를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] evaluate_suppliers(supplier_name={supplier_name}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.evaluate_suppliers\", f\"supplier_name={supplier_name}\")\n",
        "    return \"supplier_evaluation_complete\"\n",
        "\n",
        "@tool\n",
        "def handle_compliance(compliance_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"규제 준수, 세관, 문서화 및 인증을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] handle_compliance(compliance_type={compliance_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.handle_compliance\", f\"compliance_type={compliance_type}\")\n",
        "    return \"compliance_handled\"\n",
        "\n",
        "SUPPLIER_TOOLS = [evaluate_suppliers, handle_compliance, send_logistics_response]\n",
        "\n",
        "# 메인 프로세스용 LLM (슈퍼바이저용)\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", callbacks=[StreamingStdOutCallbackHandler()], verbose=True)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    operation: Optional[dict]  # 공급망 운영 정보\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "# 도구 이름으로 도구 맵 생성\n",
        "TOOLS_MAP = {\n",
        "    \"inventory\": INVENTORY_TOOLS,\n",
        "    \"transportation\": TRANSPORTATION_TOOLS,\n",
        "    \"supplier\": SUPPLIER_TOOLS,\n",
        "}\n",
        "\n",
        "# 전문가를 위한 Ray 액터 (세션별 격리)\n",
        "# 주의: LLM 객체는 직렬화가 불가능하므로 액터 내부에서 생성해야 함\n",
        "@ray.remote\n",
        "class SpecialistActor:\n",
        "    def __init__(self, name: str, tools_key: str, system_prompt: str):\n",
        "        self.name = name\n",
        "        # 액터 내부에서 LLM 초기화 (직렬화 문제 회피)\n",
        "        base_llm = init_chat_model(model=\"gpt-5-mini\", verbose=True)\n",
        "        tools = TOOLS_MAP[tools_key]\n",
        "        self.llm = base_llm.bind_tools(tools)\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.prompt = system_prompt\n",
        "        self.internal_state = {}  # 세션별 격리된 상태, 예: 세션 내 추적용\n",
        "\n",
        "    def process_task(self, operation: dict, messages: Sequence[BaseMessage]):\n",
        "        if not operation:\n",
        "            operation = {\"operation_id\": \"알 수 없음\", \"type\": \"일반\", \"priority\": \"중간\", \"status\": \"활성\"}\n",
        "        operation_json = json.dumps(operation, ensure_ascii=False)\n",
        "        full_prompt = self.prompt + f\"\\n\\n작업: {operation_json}\"\n",
        "        \n",
        "        full = [SystemMessage(content=full_prompt)] + messages\n",
        "\n",
        "        first = self.llm.invoke(full)\n",
        "        result_messages = [first]\n",
        "\n",
        "        if hasattr(first, \"tool_calls\"):\n",
        "            for tc in first.tool_calls:\n",
        "                print(first)\n",
        "                print(tc['name'])\n",
        "                fn = self.tools.get(tc['name'])\n",
        "                if fn:\n",
        "                    out = fn.invoke(tc[\"args\"])\n",
        "                    result_messages.append(ToolMessage(content=str(out), tool_call_id=tc[\"id\"]))\n",
        "\n",
        "            second = self.llm.invoke(full + result_messages)\n",
        "            result_messages.append(second)\n",
        "\n",
        "        # 내부 상태 업데이트 (예: 세션 내에서 처리된 단계 추적)\n",
        "        step_key = str(len(self.internal_state) + 1)  # 또는 더 구체적인 키 사용\n",
        "        self.internal_state[step_key] = {\"status\": \"처리됨\", \"timestamp\": time.time()}\n",
        "\n",
        "        return {\"messages\": result_messages}\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.internal_state  # 전체 세션 상태 반환\n",
        "\n",
        "# 세션 관리자 액터: 세션별 전문가 액터를 추적\n",
        "@ray.remote\n",
        "class SessionManager:\n",
        "    def __init__(self):\n",
        "        self.sessions: Dict[str, Dict[str, ray.actor.ActorHandle]] = {}  # session_id -> {agent_name: actor}\n",
        "\n",
        "    def get_or_create_actor(self, session_id: str, agent_name: str, prompt: str):\n",
        "        if session_id not in self.sessions:\n",
        "            self.sessions[session_id] = {}\n",
        "        if agent_name not in self.sessions[session_id]:\n",
        "            # LLM은 액터 내부에서 생성됨 (직렬화 불가능하므로)\n",
        "            actor = SpecialistActor.remote(agent_name, agent_name, prompt)\n",
        "            self.sessions[session_id][agent_name] = actor\n",
        "        return self.sessions[session_id][agent_name]\n",
        "\n",
        "    def get_session_state(self, session_id: str, agent_name: str):\n",
        "        if session_id in self.sessions and agent_name in self.sessions[session_id]:\n",
        "            actor = self.sessions[session_id][agent_name]\n",
        "            return actor.get_state.remote()  # future 반환\n",
        "        return None\n",
        "\n",
        "# 슈퍼바이저: 전문가를 결정하고 관리자를 통해 세션별 Ray 액터를 원격으로 호출\n",
        "def supervisor_invoke(operation: dict, messages: Sequence[BaseMessage], manager: ray.actor.ActorHandle, prompts: dict):\n",
        "    session_id = operation.get(\"operation_id\", \"알 수 없음\")\n",
        "    operation_json = json.dumps(operation, ensure_ascii=False)\n",
        "    \n",
        "    supervisor_prompt = (\n",
        "        \"당신은 공급망 전문가 팀을 조정하는 슈퍼바이저입니다.\\n\"\n",
        "        \"팀원:\\n\"\n",
        "        \"- inventory: 재고 수준, 예측, 품질, 창고 최적화, 확장 및 비용을 처리합니다.\\n\"\n",
        "        \"- transportation: 배송 추적, 준비, 운영 조정, 특수 처리, 반품, 배송 최적화 및 중단을 처리합니다.\\n\"\n",
        "        \"- supplier: 공급업체 평가 및 규정 준수를 처리합니다.\\n\"\n",
        "        \"\\n\"\n",
        "        \"사용자 쿼리를 기반으로 처리할 팀원 한 명을 선택하세요.\\n\"\n",
        "        \"선택한 팀원의 이름(inventory, transportation 또는 supplier)만 출력하고 다른 것은 출력하지 마세요.\\n\\n\"\n",
        "        f\"작업: {operation_json}\"\n",
        "    )\n",
        "\n",
        "    full = [SystemMessage(content=supervisor_prompt)] + messages\n",
        "    response = llm.invoke(full)\n",
        "    agent_name = response.content.strip().lower()\n",
        "    \n",
        "    if agent_name not in prompts:\n",
        "        raise ValueError(f\"알 수 없는 에이전트: {agent_name}\")\n",
        "    \n",
        "    # 세션별 액터 가져오기 또는 생성 (LLM은 액터 내부에서 생성됨)\n",
        "    actor_ref = manager.get_or_create_actor.remote(\n",
        "        session_id, agent_name, prompts[agent_name]\n",
        "    )\n",
        "    actor = ray.get(actor_ref)  # 액터 핸들 가져오기\n",
        "    \n",
        "    # 원격 호출\n",
        "    result_ref = actor.process_task.remote(operation, messages)\n",
        "    result = ray.get(result_ref)\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Traceloop.init(disable_batch=True, app_name=\"supply_chain_logistics_agent_ray_per_session\")\n",
        "    ray.init(ignore_reinit_error=True)  # 데모용 로컬 클러스터; 분산을 위해 구성\n",
        "\n",
        "    # 프롬프트 정의 (원본과 동일)\n",
        "    inventory_prompt = (\n",
        "        \"당신은 재고 및 창고 관리 전문가입니다.\\n\"\n",
        "        \"관리할 때:\\n\"\n",
        "        \"  1) 재고/창고 과제를 분석합니다\\n\"\n",
        "        \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "        \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "        \"비용, 효율성 및 확장성을 고려하세요.\"\n",
        "    )\n",
        "    transportation_prompt = (\n",
        "        \"당신은 운송 및 물류 전문가입니다.\\n\"\n",
        "        \"관리할 때:\\n\"\n",
        "        \"  1) 배송/전달 과제를 분석합니다\\n\"\n",
        "        \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "        \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "        \"효율성, 지속가능성 및 위험 완화를 고려하세요.\"\n",
        "    )\n",
        "    supplier_prompt = (\n",
        "        \"당신은 공급업체 관계 및 규정 준수 전문가입니다.\\n\"\n",
        "        \"관리할 때:\\n\"\n",
        "        \"  1) 공급업체/규정 준수 문제를 분석합니다\\n\"\n",
        "        \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "        \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "        \"성과, 규정 및 관계를 고려하세요.\"\n",
        "    )\n",
        "\n",
        "    prompts = {\n",
        "        \"inventory\": inventory_prompt,\n",
        "        \"transportation\": transportation_prompt,\n",
        "        \"supplier\": supplier_prompt\n",
        "    }\n",
        "\n",
        "    # 세션 관리자 생성\n",
        "    manager = SessionManager.remote()\n",
        "\n",
        "    # 예시 호출\n",
        "    example = {\"operation_id\": \"OP-12345\", \"type\": \"inventory_management\", \"priority\": \"high\", \"location\": \"Warehouse A\"}\n",
        "    convo = [HumanMessage(content=\"SKU-12345 재고가 심각하게 부족합니다. 현재 재고는 50개이지만 미처리 주문이 200개입니다. 재주문 전략은 무엇입니까?\")]\n",
        "\n",
        "    result = supervisor_invoke(example, convo, manager, prompts)\n",
        "    for m in result[\"messages\"]:\n",
        "        print(f\"{m.type}: {m.content}\")\n",
        "\n",
        "    # 선택 사항: 세션별 액터 상태 쿼리\n",
        "    state_ref = manager.get_session_state.remote(\"OP-12345\", \"inventory\")\n",
        "    if state_ref:\n",
        "        state = ray.get(ray.get(state_ref))  # 중첩된 future 해결\n",
        "        print(\"세션 액터 상태:\", state)\n",
        "\n",
        "    # Ray 종료\n",
        "    ray.shutdown()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. temporal_supply_chain_multi_agent.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 코드는 Temporal 서버/워커 구성이 필요합니다. Colab에서는 실행이 제한될 수 있으니 원본 파일을 참고해 로컬에서 실행하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "시간 확장 공급망 멀티 에이전트 예제를 구성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\"\"\"\n",
        "supply_chain_logistics_agent_temporal.py\n",
        "내구성 있는 오케스트레이션을 위해 Temporal을 사용하는 멀티 에이전트 공급망 및 물류 관리 시스템을 위한 LangGraph 워크플로우.\n",
        "Temporal 워크플로우를 통해 오케스트레이션되는 전문 에이전트를 통해 재고 관리, 운송 작업, 공급업체 관계 및 창고 최적화를 처리합니다.\n",
        "워크플로우는 재시도, 영구 상태 및 장애 복구를 통해 에이전트 단계를 순차적으로 실행하며, 장기 실행 공급망 프로세스에 이상적입니다.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import timedelta\n",
        "from typing import Annotated, Sequence, TypedDict, Optional, Dict, Any\n",
        "\n",
        "from temporalio import workflow, activity\n",
        "from temporalio.common import RetryPolicy\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from temporalio.client import Client\n",
        "from temporalio.worker import Worker\n",
        "from temporalio.worker.workflow_sandbox import SandboxedWorkflowRunner, SandboxRestrictions, SandboxMatcher\n",
        "\n",
        "from traceloop.sdk import Traceloop\n",
        "from src.common.observability.loki_logger import log_to_loki\n",
        "\n",
        "# 환경변수 확인\n",
        "import os\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  \n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY가 설정되지 않았습니다.\"\n",
        "        \"환경변수 또는 .env 파일에서 설정해주세요.\"\n",
        "    )\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4317\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_INSECURE\"] = \"true\"\n",
        "\n",
        "def ensure_message(m):\n",
        "    if isinstance(m, BaseMessage):\n",
        "        return m\n",
        "    if isinstance(m, dict):\n",
        "        msg_type = m.get(\"type\")\n",
        "        content = m.get(\"content\")\n",
        "        # 생성자에 전달할 kwargs에서 type 제거\n",
        "        kwargs = {k:v for k,v in m.items() if k not in [\"type\", \"content\"]}\n",
        "        \n",
        "        if msg_type == \"human\":\n",
        "            return HumanMessage(content=content, **kwargs)\n",
        "        elif msg_type == \"ai\":\n",
        "            return AIMessage(content=content, **kwargs)\n",
        "        elif msg_type == \"system\":\n",
        "            return SystemMessage(content=content, **kwargs)\n",
        "        elif msg_type == \"tool\":\n",
        "            return ToolMessage(content=content, **kwargs)\n",
        "        return HumanMessage(content=content, **kwargs)\n",
        "    return HumanMessage(content=str(m))\n",
        "\n",
        "# 모든 전문가를 위한 공유 도구\n",
        "@tool\n",
        "def send_logistics_response(operation_id: Optional[str] = None, message: Optional[str] = None) -> str:\n",
        "    \"\"\"이해관계자에게 물류 업데이트, 권장 사항 또는 상태 보고서를 전송합니다.\"\"\"\n",
        "    print(f\"[도구] send_logistics_response → {message}\")\n",
        "    log_to_loki(\"tool.send_logistics_response\", f\"operation_id={operation_id}, message={message}\")\n",
        "    return \"logistics_response_sent\"\n",
        "\n",
        "# 재고 및 창고 전문가 도구\n",
        "@tool\n",
        "def manage_inventory(sku: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"재고 수준, 재고 보충, 감사 및 최적화 전략을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_inventory(sku={sku}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_inventory\", f\"sku={sku}\")\n",
        "    return \"inventory_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def optimize_warehouse(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"창고 운영, 레이아웃, 용량 및 보관 효율성을 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_warehouse(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_warehouse\", f\"operation_type={operation_type}\")\n",
        "    return \"warehouse_optimization_initiated\"\n",
        "\n",
        "@tool\n",
        "def forecast_demand(season: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"수요 패턴, 계절적 추세를 분석하고 예측 모델을 생성합니다.\"\"\"\n",
        "    print(f\"[도구] forecast_demand(season={season}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.forecast_demand\", f\"season={season}\")\n",
        "    return \"demand_forecast_generated\"\n",
        "\n",
        "@tool\n",
        "def manage_quality(supplier: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"품질 관리, 결함 추적 및 공급업체 품질 표준을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_quality(supplier={supplier}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_quality\", f\"supplier={supplier}\")\n",
        "    return \"quality_management_initiated\"\n",
        "\n",
        "@tool\n",
        "def scale_operations(scaling_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"성수기, 용량 계획 및 인력 관리를 위한 운영을 확장합니다.\"\"\"\n",
        "    print(f\"[도구] scale_operations(scaling_type={scaling_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.scale_operations\", f\"scaling_type={scaling_type}\")\n",
        "    return \"operations_scaled\"\n",
        "\n",
        "@tool\n",
        "def optimize_costs(cost_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"운송, 보관 및 운영 비용을 분석하고 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_costs(cost_type={cost_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_costs\", f\"cost_type={cost_type}\")\n",
        "    return \"cost_optimization_initiated\"\n",
        "\n",
        "INVENTORY_TOOLS = [manage_inventory, optimize_warehouse, forecast_demand, manage_quality, scale_operations, optimize_costs, send_logistics_response]\n",
        "\n",
        "# 운송 및 물류 전문가 도구\n",
        "@tool\n",
        "def track_shipments(origin: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 상태, 지연 사항을 추적하고 배송 물류를 조정합니다.\"\"\"\n",
        "    print(f\"[도구] track_shipments(origin={origin}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.track_shipments\", f\"origin={origin}\")\n",
        "    return \"shipment_tracking_updated\"\n",
        "\n",
        "@tool\n",
        "def arrange_shipping(shipping_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 방법, 특급 배송 및 복합 운송을 준비합니다.\"\"\"\n",
        "    print(f\"[도구] arrange_shipping(shipping_type={shipping_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.arrange_shipping\", f\"shipping_type={shipping_type}\")\n",
        "    return \"shipping_arranged\"\n",
        "\n",
        "@tool\n",
        "def coordinate_operations(operation_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"크로스도킹, 통합 및 이동과 같은 복잡한 작업을 조정합니다.\"\"\"\n",
        "    print(f\"[도구] coordinate_operations(operation_type={operation_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.coordinate_operations\", f\"operation_type={operation_type}\")\n",
        "    return \"operations_coordinated\"\n",
        "\n",
        "@tool\n",
        "def manage_special_handling(product_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"위험물, 콜드체인 및 민감한 제품에 대한 특수 요구사항을 처리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_special_handling(product_type={product_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_special_handling\", f\"product_type={product_type}\")\n",
        "    return \"special_handling_managed\"\n",
        "\n",
        "@tool\n",
        "def process_returns(returned_quantity: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"반품, 역물류 및 제품 처리를 처리합니다.\"\"\"\n",
        "    print(f\"[도구] process_returns(returned_quantity={returned_quantity}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.process_returns\", f\"returned_quantity={returned_quantity}\")\n",
        "    return \"returns_processed\"\n",
        "\n",
        "@tool\n",
        "def optimize_delivery(delivery_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"배송 경로, 라스트마일 물류 및 지속가능성 이니셔티브를 최적화합니다.\"\"\"\n",
        "    print(f\"[도구] optimize_delivery(delivery_type={delivery_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.optimize_delivery\", f\"delivery_type={delivery_type}\")\n",
        "    return \"delivery_optimization_complete\"\n",
        "\n",
        "@tool\n",
        "def manage_disruption(disruption_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급망 중단, 비상 계획 및 위험 완화를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] manage_disruption(disruption_type={disruption_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.manage_disruption\", f\"disruption_type={disruption_type}\")\n",
        "    return \"disruption_managed\"\n",
        "\n",
        "TRANSPORTATION_TOOLS = [track_shipments, arrange_shipping, coordinate_operations, manage_special_handling, process_returns, optimize_delivery, manage_disruption, send_logistics_response]\n",
        "\n",
        "# 공급업체 및 규정 준수 전문가 도구\n",
        "@tool\n",
        "def evaluate_suppliers(supplier_name: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"공급업체 성과를 평가하고 감사를 수행하며 공급업체 관계를 관리합니다.\"\"\"\n",
        "    print(f\"[도구] evaluate_suppliers(supplier_name={supplier_name}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.evaluate_suppliers\", f\"supplier_name={supplier_name}\")\n",
        "    return \"supplier_evaluation_complete\"\n",
        "\n",
        "@tool\n",
        "def handle_compliance(compliance_type: Optional[str] = None, **kwargs) -> str:\n",
        "    \"\"\"규제 준수, 세관, 문서화 및 인증을 관리합니다.\"\"\"\n",
        "    print(f\"[도구] handle_compliance(compliance_type={compliance_type}, kwargs={kwargs})\")\n",
        "    log_to_loki(\"tool.handle_compliance\", f\"compliance_type={compliance_type}\")\n",
        "    return \"compliance_handled\"\n",
        "\n",
        "SUPPLIER_TOOLS = [evaluate_suppliers, handle_compliance, send_logistics_response]\n",
        "\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", temperature=0.0, callbacks=[StreamingStdOutCallbackHandler()], verbose=True)\n",
        "\n",
        "# 전문화된 LLM에 도구 바인딩\n",
        "inventory_llm = llm.bind_tools(INVENTORY_TOOLS)\n",
        "transportation_llm = llm.bind_tools(TRANSPORTATION_TOOLS)\n",
        "supplier_llm = llm.bind_tools(SUPPLIER_TOOLS)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    operation: Optional[dict]  # 공급망 운영 정보\n",
        "    messages: Annotated[Sequence[BaseMessage], \"add\"]\n",
        "\n",
        "# Temporal 액티비티 (전문가 로직 래핑)\n",
        "@activity.defn\n",
        "async def supervisor_activity(operation: Dict[str, Any], messages: list) -> Dict[str, Any]:\n",
        "    \"\"\"슈퍼바이저를 통해 전문가를 결정하는 액티비티.\"\"\"\n",
        "    operation_json = json.dumps(operation, ensure_ascii=False)\n",
        "    \n",
        "    supervisor_prompt = (\n",
        "        \"당신은 공급망 전문가 팀을 조정하는 슈퍼바이저입니다.\\n\"\n",
        "        \"팀원:\\n\"\n",
        "        \"- inventory: 재고 수준, 예측, 품질, 창고 최적화, 확장 및 비용을 처리합니다.\\n\"\n",
        "        \"- transportation: 배송 추적, 준비, 운영 조정, 특수 처리, 반품, 배송 최적화 및 중단을 처리합니다.\\n\"\n",
        "        \"- supplier: 공급업체 평가 및 규정 준수를 처리합니다.\\n\"\n",
        "        \"\\n\"\n",
        "        \"사용자 쿼리를 기반으로 처리할 팀원 한 명을 선택하세요.\\n\"\n",
        "        \"선택한 팀원의 이름(inventory, transportation 또는 supplier)만 출력하고 다른 것은 출력하지 마세요.\\n\\n\"\n",
        "        f\"작업: {operation_json}\"\n",
        "    )\n",
        "\n",
        "    full = [SystemMessage(content=supervisor_prompt)] + [ensure_message(m) for m in messages]\n",
        "    response = llm.invoke(full)\n",
        "    agent_name = response.content.strip().lower()\n",
        "    return {\"agent_name\": agent_name, \"messages\": [response.dict()]}\n",
        "\n",
        "@activity.defn\n",
        "async def specialist_activity(agent_name: str, operation: Dict[str, Any], messages: list) -> Dict[str, Any]:\n",
        "    \"\"\"전문가 처리를 위한 액티비티 (inventory, transportation, supplier).\"\"\"\n",
        "    if agent_name not in prompts:\n",
        "        raise ValueError(f\"알 수 없는 에이전트: {agent_name}\")\n",
        "    \n",
        "    specialist_llm = llms_dict[agent_name]\n",
        "    tools = {t.name: t for t in tools_dict[agent_name]}\n",
        "    system_prompt = prompts[agent_name]\n",
        "    \n",
        "    operation_json = json.dumps(operation, ensure_ascii=False)\n",
        "    full_prompt = system_prompt + f\"\\n\\n작업: {operation_json}\"\n",
        "    \n",
        "    full = [SystemMessage(content=full_prompt)] + [ensure_message(m) for m in messages]\n",
        "\n",
        "    first = specialist_llm.invoke(full)\n",
        "    result_messages = [first.dict()]\n",
        "\n",
        "    if hasattr(first, \"tool_calls\"):\n",
        "        for tc in first.tool_calls:\n",
        "            fn = tools.get(tc['name'])\n",
        "            if fn:\n",
        "                out = fn.invoke(tc[\"args\"])\n",
        "                result_messages.append(ToolMessage(content=str(out), tool_call_id=tc[\"id\"]).dict())\n",
        "\n",
        "        second = specialist_llm.invoke(full + [ensure_message(msg) for msg in result_messages])\n",
        "        result_messages.append(second.dict())\n",
        "\n",
        "    return {\"messages\": result_messages}\n",
        "\n",
        "# Temporal 워크플로우\n",
        "@workflow.defn(name=\"SupplyChainWorkflow\")\n",
        "class SupplyChainWorkflow:\n",
        "    @workflow.run\n",
        "    async def run(self, operation: Dict[str, Any], initial_messages: list) -> Dict[str, Any]:\n",
        "        # 단계 1: 슈퍼바이저가 라우팅\n",
        "        supervisor_result = await workflow.execute_activity(\n",
        "            supervisor_activity,\n",
        "            args=[operation, initial_messages],\n",
        "            start_to_close_timeout=timedelta(seconds=60),\n",
        "            retry_policy=RetryPolicy(maximum_attempts=3)\n",
        "        )\n",
        "        agent_name = supervisor_result[\"agent_name\"]\n",
        "        updated_messages = initial_messages + supervisor_result[\"messages\"]\n",
        "        \n",
        "        # 단계 2: 전문가 처리\n",
        "        specialist_result = await workflow.execute_activity(\n",
        "            specialist_activity,\n",
        "            args=[agent_name, operation, updated_messages],\n",
        "            start_to_close_timeout=timedelta(seconds=60),\n",
        "            retry_policy=RetryPolicy(maximum_attempts=3)\n",
        "        )\n",
        "        \n",
        "        # 결과 컴파일 (필요시 다단계로 확장)\n",
        "        final_messages = updated_messages + specialist_result[\"messages\"]\n",
        "        return {\n",
        "            \"agent_name\": agent_name,\n",
        "            \"final_messages\": final_messages,\n",
        "            \"operation\": operation\n",
        "        }\n",
        "\n",
        "# 프롬프트 정의\n",
        "inventory_prompt = (\n",
        "    \"당신은 재고 및 창고 관리 전문가입니다.\\n\"\n",
        "    \"관리할 때:\\n\"\n",
        "    \"  1) 재고/창고 과제를 분석합니다\\n\"\n",
        "    \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "    \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "    \"비용, 효율성 및 확장성을 고려하세요.\"\n",
        ")\n",
        "\n",
        "transportation_prompt = (\n",
        "    \"당신은 운송 및 물류 전문가입니다.\\n\"\n",
        "    \"관리할 때:\\n\"\n",
        "    \"  1) 배송/전달 과제를 분석합니다\\n\"\n",
        "    \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "    \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "    \"효율성, 지속가능성 및 위험 완화를 고려하세요.\"\n",
        ")\n",
        "\n",
        "supplier_prompt = (\n",
        "    \"당신은 공급업체 관계 및 규정 준수 전문가입니다.\\n\"\n",
        "    \"관리할 때:\\n\"\n",
        "    \"  1) 공급업체/규정 준수 문제를 분석합니다\\n\"\n",
        "    \"  2) 적절한 도구를 호출합니다\\n\"\n",
        "    \"  3) send_logistics_response로 후속 조치합니다\\n\"\n",
        "    \"성과, 규정 및 관계를 고려하세요.\"\n",
        ")\n",
        "\n",
        "prompts = {\n",
        "    \"inventory\": inventory_prompt,\n",
        "    \"transportation\": transportation_prompt,\n",
        "    \"supplier\": supplier_prompt\n",
        "}\n",
        "\n",
        "llms_dict = {\n",
        "    \"inventory\": inventory_llm,\n",
        "    \"transportation\": transportation_llm,\n",
        "    \"supplier\": supplier_llm\n",
        "}\n",
        "\n",
        "tools_dict = {\n",
        "    \"inventory\": INVENTORY_TOOLS,\n",
        "    \"transportation\": TRANSPORTATION_TOOLS,\n",
        "    \"supplier\": SUPPLIER_TOOLS\n",
        "}\n",
        "\n",
        "async def main():\n",
        "    client = await Client.connect(\"localhost:7233\")\n",
        "    \n",
        "    # 샌드박스 설정: LangChain 등 외부 라이브러리 허용\n",
        "    runner = SandboxedWorkflowRunner(\n",
        "        restrictions=SandboxRestrictions(\n",
        "            invalid_modules=SandboxMatcher(),\n",
        "            invalid_module_members=SandboxRestrictions.invalid_module_members_default,\n",
        "            passthrough_modules=SandboxRestrictions.passthrough_modules_default | {\n",
        "                \"langchain\", \n",
        "                \"langchain_core\", \n",
        "                \"langchain_community\", \n",
        "                \"langchain_openai\",\n",
        "                \"pydantic\", \n",
        "                \"requests\", \n",
        "                \"urllib3\", \n",
        "                \"http\",\n",
        "                \"certifi\",\n",
        "                \"charset_normalizer\",\n",
        "                \"idna\",\n",
        "                \"ssl\",\n",
        "                \"socket\",\n",
        "                \"logging\",\n",
        "                \"tenacity\",\n",
        "                \"traceloop\"\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 워커 시작\n",
        "    async with Worker(\n",
        "        client, \n",
        "        task_queue=\"supply-chain-queue\", \n",
        "        workflows=[SupplyChainWorkflow], \n",
        "        activities=[supervisor_activity, specialist_activity],\n",
        "        workflow_runner=runner\n",
        "    ):\n",
        "        # 예시 실행\n",
        "        example_operation = {\"operation_id\": \"OP-12345\", \"type\": \"inventory_management\", \"priority\": \"high\", \"location\": \"Warehouse A\"}\n",
        "        example_messages = [{\"content\": \"SKU-12345 재고가 심각하게 부족합니다. 현재 재고는 50개이지만 미처리 주문이 200개입니다. 재주문 전략은 무엇입니까?\", \"type\": \"human\"}]\n",
        "\n",
        "        result = await client.execute_workflow(\n",
        "            SupplyChainWorkflow.run,\n",
        "            args=[example_operation, example_messages],\n",
        "            id=\"supply-chain-workflow\",\n",
        "            task_queue=\"supply-chain-queue\"\n",
        "        )\n",
        "        print(\"워크플로우 결과:\")\n",
        "        for m in result[\"final_messages\"]:\n",
        "            print(m)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    Traceloop.init(disable_batch=True, app_name=\"supply_chain_logistics_agent_temporal\")\n",
        "    asyncio.run(main())\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}