{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 06: 메모리 (Memory)\n",
        "\n",
        "이 노트북에서는 에이전트의 단기/장기 메모리 구현을 학습합니다.\n",
        "\n",
        "## 주요 내용\n",
        "- 단기 메모리\n",
        "- 메모리 유지 대화\n",
        "- 체크포인터 활용\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/Building-Applications-with-AI-Agents/blob/main/notebook/ch06_memory.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 패키지 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langgraph python-dotenv rank-bm25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API 키 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✅ Colab Secrets에서 API 키를 불러왔습니다.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
        "    print(\"⚠️ API 키를 직접 입력해주세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. short_term_memory.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "메모리 유무에 따른 대화 차이를 보여줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "\n",
        "# 환경변수 확인\n",
        "import os\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  \n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY가 설정되지 않았습니다.\"\n",
        "        \"환경변수 또는 .env 파일에서 설정해주세요.\"\n",
        "    )\n",
        "\n",
        "# LLM 초기화\n",
        "llm = init_chat_model(model=\"gpt-5-mini\", temperature=0)\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"call_model\", call_model)\n",
        "builder.add_edge(START, \"call_model\")\n",
        "graph = builder.compile()\n",
        "\n",
        "# 메모리가 없어서 대화 상태를 유지할 수 없음\n",
        "input_message = {\"type\": \"user\", \"content\": \"안녕하세요! 제 이름은 민혁입니다.\"}\n",
        "for chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "\n",
        "input_message = {\"type\": \"user\", \"content\": \"제 이름이 뭐라고 했죠?\"}\n",
        "for chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# 메모리가 유지되어 대화 상태를 유지할 수 있음\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "input_message = {\"type\": \"user\", \"content\": \"안녕하세요! 제 이름은 민혁입니다.\"}\n",
        "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. basic_bm25.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BM25로 간단한 텍스트 검색을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List\n",
        "\n",
        "corpus: List[List[str]] = [\n",
        "    \"에이전트 J는 패기가 넘치는 신입 대원이다\".split(),\n",
        "    \"에이전트 K는 수년간의 MIB 경험과 멋진 뉴럴라이저를 갖고 있다\".split(),\n",
        "    \"두 명의 에이전트가 검은 정장을 입고 은하계를 구했다\".split(),\n",
        "]\n",
        "# 2. BM25 인덱스 생성\n",
        "bm25 = BM25Okapi(corpus)\n",
        "\n",
        "# 3. 간단한 쿼리로 검색 수행\n",
        "query = \"신입 대원은 누구지?\".split()\n",
        "top_n = bm25.get_top_n(query, corpus, n=2)\n",
        "\n",
        "print(\"쿼리:\", \" \".join(query))\n",
        "print(\"상위 일치 문장:\")\n",
        "for line in top_n:\n",
        "    print(\" •\", \" \".join(line))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}