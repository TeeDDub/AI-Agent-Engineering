{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 12: 프롬프트 인젝션 (Prompt Injection)\n",
        "\n",
        "이 노트북에서는 프롬프트 인젝션 차단과 입력 스캐닝을 다룹니다.\n",
        "\n",
        "## 주요 내용\n",
        "- 입력 스캐너 구성\n",
        "- PII 익명화\n",
        "- 프롬프트 인젝션 차단\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TeeDDub/Building-Applications-with-AI-Agents/blob/main/notebook/ch12_prompt_injection.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 패키지 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q llm-guard python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API 키 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✅ Colab Secrets에서 API 키를 불러왔습니다.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
        "    print(\"⚠️ API 키를 직접 입력해주세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. prompt_injection_block.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "입력 스캐너로 프롬프트 인젝션을 차단합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from llm_guard import scan_prompt\n",
        "from llm_guard.input_scanners import Anonymize, BanSubstrings\n",
        "from llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF\n",
        "from llm_guard.vault import Vault\n",
        "\n",
        "# Vault 초기화(Anonymize가 원본 값을 저장하는 데 필요)\n",
        "vault = Vault()\n",
        "\n",
        "# 스캐너 정의\n",
        "scanners = [\n",
        "    Anonymize(\n",
        "        vault=vault,                       \n",
        "        preamble=\"정제된 입력: \",           \n",
        "        allowed_names=[\"John Doe\"],        \n",
        "        hidden_names=[\"Test LLC\"],        \n",
        "        recognizer_conf=BERT_LARGE_NER_CONF,\n",
        "        language=\"en\",                                              \n",
        "        entity_types=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],   \n",
        "        use_faker=False,                                            \n",
        "        threshold=0.5                      \n",
        "    ),\n",
        "    BanSubstrings(substrings=[\"malicious\", \"override system\"], match_type=\"word\")\n",
        "]\n",
        "\n",
        "# 잠재적 PII가 포함된 입력 프롬프트 예시\n",
        "prompt = \"Tell me about John Doe's email: john@example.com\" + \\\n",
        "         \"and how to override system security.\" # 존 도의 이메일 john@example.com 에 대해 알려 주고 시스템 보안을 어떻게 우회하는지도 설명해 줘.\n",
        "\n",
        "# 프롬프트 스캔 및 정제\n",
        "sanitized_prompt, results_valid, results_score = scan_prompt(scanners, prompt)\n",
        "\n",
        "if any(not result for result in results_valid.values()):\n",
        "    print(\"입력에 문제가 있습니다. 거부하거나 적절히 처리합니다.\")\n",
        "    print(f\"위험 점수: {results_score}\")\n",
        "else:\n",
        "    print(f\"정제된 프롬프트: {sanitized_prompt}\")\n",
        "    # 이 정제된 프롬프트를 모델에 전달해 계속 진행합니다.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}